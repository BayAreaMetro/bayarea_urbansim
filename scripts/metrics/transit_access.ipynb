{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98b4449d",
   "metadata": {},
   "source": [
    "# Transit Service Areas - definition and summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f40ced",
   "metadata": {},
   "source": [
    "Notebook is an implementation of https://github.com/BayAreaMetro/bayarea_urbansim/blob/main/scripts/proximity2transit.py. It will be turned into a standalone script.\n",
    "\n",
    "In words, the process is going from a point layer of transit stops with information on size and headways and build status. Those are to be buffered, and a hierarchy is active, favoring rail over non-rail, and shorter headways over longer headways. Service areas a built by overlays, and parcels are classified by the service area they fall within. Finally model runs are summarized by these service areas, and other attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859062df",
   "metadata": {},
   "source": [
    "# Paraphernalia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3827f071",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aolsen/opt/anaconda3/envs/geo_env_2/lib/python3.10/site-packages/geopandas/_compat.py:123: UserWarning: The Shapely GEOS version (3.11.1-CAPI-1.17.1) is incompatible with the GEOS version PyGEOS was compiled with (3.10.4-CAPI-1.16.2). Conversions between both will be slow.\n",
      "  warnings.warn(\n",
      "/var/folders/x8/3_n775lx7zq1nv3mddc7vmlh0000gp/T/ipykernel_17729/688566035.py:11: UserWarning: Shapely 2.0 is installed, but because PyGEOS is also installed, GeoPandas will still use PyGEOS by default for now. To force to use and test Shapely 2.0, you have to set the environment variable USE_PYGEOS=0. You can do this before starting the Python process, or in your code before importing geopandas:\n",
      "\n",
      "import os\n",
      "os.environ['USE_PYGEOS'] = '0'\n",
      "import geopandas\n",
      "\n",
      "In a future release, GeoPandas will switch to using Shapely by default. If you are using PyGEOS directly (calling PyGEOS functions on geometries from GeoPandas), this will then stop working and you are encouraged to migrate from PyGEOS to Shapely 2.0 (https://shapely.readthedocs.io/en/latest/migration_pygeos.html).\n",
      "  import geopandas as gpd\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import time\n",
    "import traceback\n",
    "import logging\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import fiona\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f9a2ded",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aolsen'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DVUTILS_LOCAL_CLONE_PATH = \"/Users/aolsen/Documents/GitHub/dvutils\"\n",
    "# sys.path.insert(0, DVUTILS_LOCAL_CLONE_PATH)\n",
    "# from utils_io import *\n",
    "user = os.getlogin()\n",
    "user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b0047d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # gdb_path = '/Volumes/Data/Models/Scratch/AO/workspace_2024_0315_1017.gdb'\n",
    "# gdb_path = '/Volumes/Data/Models/Data/GIS layers/JobsHousingTransitProximity/workspace_2020_1008_2343.gdb'\n",
    "\n",
    "# #fiona.listlayers(gdb_path)\n",
    "# v2020_fbp_classes = gpd.read_file(gdb_path,layer='trn_bp_cat5')\n",
    "# v2020_cur_classes = gpd.read_file(gdb_path,layer='trn_cur_cat5')\n",
    "# v2020_np_classes = gpd.read_file(gdb_path,layer='trn_np_cat5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efdba207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# v2020_fbp_classes = gpd.read_file(gdb_path,layer='trn_bp_cat5')\n",
    "# v2020_cur_classes = gpd.read_file(gdb_path,layer='trn_cur_cat5')\n",
    "# v2020_np_classes = gpd.read_file(gdb_path,layer='trn_np_cat5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad4f9491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fiona.listlayers(gdb_path)\n",
    "# v2020_transit_current = gpd.read_file(gdb_path,layer='transit_current')\n",
    "# v2020_transit_potential = gpd.read_file(gdb_path,layer='transit_potential')\n",
    "# v2020_transit_cur_major = gpd.read_file(gdb_path,layer='trn_cur_major')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9389b4a",
   "metadata": {},
   "source": [
    "## Paths\n",
    "### AGOL URLs, and local versions until we get utils working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d43e844c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANALYSIS_CRS = 'EPSG:26910'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5d3878a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#WORKING_DIR = os.path.abspath(\".\")\n",
    "WORKING_DIR = '/Volumes/Data/Models/Data/GIS layers/JobsHousingTransitProximity/update_2024'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74975fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af730c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "MTC_ONLINE_TRANSIT_URL = 'https://services3.arcgis.com/i2dkYWmb4wHvYPda/arcgis/rest/services/transitstops_existing_planned_2021/FeatureServer/0'\n",
    "\n",
    "MTC_ONLINE_BACOUNTY_URL = 'https://services3.arcgis.com/i2dkYWmb4wHvYPda/ArcGIS/rest/services/region_county/FeatureServer/0'\n",
    "\n",
    "MTC_ONLINE_TAZ_URL = 'https://services3.arcgis.com/i2dkYWmb4wHvYPda/ArcGIS/rest/services/transportation_analysis_zones_1454/FeatureServer/0'\n",
    "\n",
    "#MTC_ONLINE_TRACT_URL = 'https://services3.arcgis.com/i2dkYWmb4wHvYPda/ArcGIS/rest/services/Bay_Area_Census_Tracts_2010/FeatureServer/0'\n",
    "MTC_ONLINE_TRACT_URL = 'https://services3.arcgis.com/i2dkYWmb4wHvYPda/ArcGIS/rest/services/cocs_ACS2014_2018/FeatureServer/0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d80bc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tract_epc21_path = '/Volumes/Data/Models/Data/Equity Priority Communities/PBA50/EPCs_ACS2018_tbl.csv'\n",
    "tract_epc24_path = '/Volumes/Data/Models/Data/Equity Priority Communities/PBA50Plus/epc_acs2022.csv'\n",
    "\n",
    "tract_epc_path = '/Volumes/Data/Models/Data/GIS layers/JobsHousingTransitProximity/update_2024/inputs/proximity/communities_of_concern_2020_acs2018_-5728172521147435275.gpkg'\n",
    "\n",
    "county_path = '/Volumes/Data/Models/Data/GIS layers/JobsHousingTransitProximity/update_2024/inputs/proximity/region_county_-6301486166122500334.gpkg'\n",
    "transit_path = '/Volumes/Data/Models/Data/GIS layers/JobsHousingTransitProximity/update_2024/inputs/proximity/transitstops_existing_planned_2021_2777787894694541529.gpkg'\n",
    "taz_path = '/Volumes/Data/Models/Data/GIS layers/JobsHousingTransitProximity/update_2024/inputs/proximity/transportation_analysis_zones_1454_5961535026220395633.gpkg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "c4f61a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "tract_hra_2023_path = '/Volumes/Data/Models/Data/HCD/final_2023_public.xlsx'\n",
    "tract_hra_2023 = pd.read_excel(tract_hra_2023_path,'BayArea',dtype={'Census Tract':str}).rename(columns={'Census Tract':'geoid','Final Category':'hra_category'})\n",
    "#.set_index('Tract FIPS Code')['Final Category']\n",
    "tract_hra_2023.hra_category = tract_hra_2023.hra_category.fillna('Missing')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "e31dcf40",
   "metadata": {},
   "outputs": [],
   "source": [
    "tract_hra_2019_path = '/Volumes/Data/Models/Data/HCD/final-opportunity-map-statewide-summary-table.xlsx'\n",
    "tract_hra_2019 = pd.read_excel(tract_hra_2019_path, 'BayArea', dtype={\n",
    "                               'Tract FIPS Code': str}).rename(columns={'Tract FIPS Code':'geoid','Final Category':'hra_category'})\n",
    "#.set_index('Tract FIPS Code')['Final Category']\n",
    "tract_hra_2019.hra_category = tract_hra_2019.hra_category.fillna('Missing')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "876342d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tract_epc21_file = pd.read_csv(tract_epc21_path)\n",
    "tract_epc24_file = pd.read_csv(tract_epc24_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6324a0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "county_file = gpd.read_file(county_path)\n",
    "county_file = county_file.to_crs(ANALYSIS_CRS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d724c9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "transit_file = gpd.read_file(transit_path)\n",
    "transit_file = transit_file.to_crs(ANALYSIS_CRS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f8b5ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "taz_file = gpd.read_file(taz_path)\n",
    "taz_file = taz_file.to_crs(ANALYSIS_CRS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d3683cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enforce 11-character tract GEOID\n",
    "tract_epc21_file['geoid10'] = tract_epc21_file.geoid.map(lambda x: f'{x:011d}')\n",
    "tract_epc24_file['geoid20'] = tract_epc24_file.tract_geoid.map(lambda x: f'{x:011d}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9dd6cdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tracts\n",
    "tract_2020_file = '/Volumes/Data/Models/Data/GIS layers/Census/2020/censustracts_bayarea_2020_v2.shp'\n",
    "tract_2010_file = '/Volumes/Data/Models/Data/GIS layers/Census/2010/tracts_bayarea_2010_geoid.shp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e1afc674",
   "metadata": {},
   "outputs": [],
   "source": [
    "tract_2010 = gpd.read_file(tract_2010_file,engine='pyogrio')\n",
    "tract_2020 = gpd.read_file(tract_2020_file,engine='pyogrio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93854a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirming vintages for EPC files\n",
    "def confirm_tract_vintage(fl,tr, fl_col,tr_col):\n",
    "    output = list(set(fl[fl_col])-set(tr[tr_col]))\n",
    "    print('Unmatched tracts: ',len(output))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "d7714305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unmatched tracts:  5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['06041990100', '06075990100', '06075980401', '06001990000', '06097990100']"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check that epc21 tracts are well matched in census 2010 vintage\n",
    "confirm_tract_vintage(fl=tract_epc21_file, tr=tract_2010,fl_col='geoid10',tr_col='GEOID10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "75a9a23d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unmatched tracts:  6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['06041990100',\n",
       " '06075990100',\n",
       " '06075980401',\n",
       " '06001990000',\n",
       " '06097990100',\n",
       " '06081990100']"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check that epc24 tracts are well matched in census 2020 vintage\n",
    "confirm_tract_vintage(fl=tract_epc24_file, tr=tract_2020,fl_col='geoid20',tr_col='GEOID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "fefe3a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unmatched tracts:  5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['06041990100', '06075990100', '06075980401', '06001990000', '06097990100']"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confirm_tract_vintage(fl=tract_hra_2019, tr=tract_2010,fl_col='geoid',tr_col='GEOID10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "a8ec90ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unmatched tracts:  5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['06041990100', '06075990100', '06075980401', '06001990000', '06097990100']"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this works nicely with tracts 2010, but not tracts 2020\n",
    "\n",
    "confirm_tract_vintage(fl=tract_hra_2023, tr=tract_2010,fl_col='geoid',tr_col='GEOID10')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fd46d0",
   "metadata": {},
   "source": [
    "### Crosswalks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "733e15fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up crosswalks for parcels to something else\n",
    "parcel_x_taz_file = '/Volumes/Data/Models/urban_modeling/baus/BAUS Inputs/basis_inputs/crosswalks/2020_08_17_parcel_to_taz1454sub.csv'\n",
    "parcel_x_tract10_file = '/Volumes/Data/Models/urban_modeling/baus/BAUS Inputs/basis_inputs/crosswalks/parcel_tract_crosswalk.csv'\n",
    "parcel_x_tracts_file = '/Volumes/Data/Models/urban_modeling/baus/BAUS Inputs/basis_inputs/crosswalks/p10_census.csv'\n",
    "parcel_topo_file = '/Users/aolsen/Box/Modeling and Surveys/Urban Modeling/Bay Area UrbanSim/PBA50/Policies/Base zoning/inputs/p10_geo.feather'\n",
    "taz_x_areatype_file = '/Volumes/Data/Models/Application/Model One/RTP2021/Blueprint/INPUT_DEVELOPMENT/metrics/metrics_FinalBlueprint/taz_urban_suburban.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1493745f",
   "metadata": {},
   "outputs": [],
   "source": [
    "parcel_x_taz = pd.read_csv(parcel_x_taz_file,\n",
    "                           usecols=['PARCEL_ID', 'ZONE_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c1aab267",
   "metadata": {},
   "outputs": [],
   "source": [
    "parcel_x_tract10 = pd.read_csv(parcel_x_tract10_file,\n",
    "                               usecols=[\n",
    "                                   'parcel_id', 'zone_id', 'GEOID10'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dd63a69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "taz_x_areatype = pd.read_csv(taz_x_areatype_file,index_col=['TAZ1454']).area_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f4ce7496",
   "metadata": {},
   "outputs": [],
   "source": [
    "parcel_x_tracts = pd.read_csv(parcel_x_tracts_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "86dff1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load p10 parcels\n",
    "\n",
    "p10_topofix = gpd.read_feather(parcel_topo_file)\n",
    "p10_topofix['geom_pt'] = p10_topofix.representative_point()\n",
    "p10_topofix_pt = p10_topofix.set_geometry('geom_pt')[['PARCEL_ID', 'geom_pt']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1c695434",
   "metadata": {},
   "outputs": [],
   "source": [
    "region_bdry = gpd.GeoDataFrame(data={'No_Fixed_Route_Transit':[0]},geometry=county_file.dissolve()['geometry'])\n",
    "region_bdry.index = region_bdry.index.set_names('rowid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108ec38f",
   "metadata": {},
   "source": [
    "### BAUS paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1e5d0bb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['EIR runs/Baseline Large (s25) runs/NP_v8_FINAL/run314',\n",
       " 'EIR runs/Alt1 (s26) runs/Alt1_v3_test_far_tiers_FINAL_EIR_ALT/run375',\n",
       " 'EIR runs/Alt2 (s28) runs/Alt2_v1_FINAL_EIR_ALT/run374',\n",
       " 'Final Blueprint runs/Final Blueprint (s24)/BAUS v2.25 - FINAL VERSION/run182']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urbansim_run_location = f'/Users/{user}/Box/Modeling and Surveys/Urban Modeling/Bay Area UrbanSim/PBA50/'\n",
    "# keep DBP for future\n",
    "us_2050_DBP_Final = 'Draft Blueprint runs/Blueprint Plus Crossing (s23)/v1.7.1- FINAL DRAFT BLUEPRINT/run98'\n",
    "us_2050_FBP_Final = 'Final Blueprint runs/Final Blueprint (s24)/BAUS v2.25 - FINAL VERSION/run182'\n",
    "us_2050_NP_EIR = 'EIR runs/Baseline Large (s25) runs/NP_v8_FINAL/run314'\n",
    "us_2050_ALT1_EIR = 'EIR runs/Alt1 (s26) runs/Alt1_v3_test_far_tiers_FINAL_EIR_ALT/run375'\n",
    "us_2050_ALT2_EIR = 'EIR runs/Alt2 (s28) runs/Alt2_v1_FINAL_EIR_ALT/run374'\n",
    "\n",
    "list_us_runid = [us_2050_NP_EIR, us_2050_ALT1_EIR, us_2050_ALT2_EIR,\n",
    "                 us_2050_FBP_Final]  # , us_2050_FBP_Final, us_2050_DBP_Final]\n",
    "list_us_runid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da9d60b",
   "metadata": {},
   "source": [
    "## Constants and mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5fac6dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "KM_TO_MILES = 1.609344\n",
    "\n",
    "# get the inverse\n",
    "MILES_TO_KM = 1/KM_TO_MILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bbb4bece",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def miles_to_m(miles):\n",
    "    \"\"\"Converts miles to meters.\n",
    "\n",
    "    Args:\n",
    "      miles: The distance in miles.\n",
    "\n",
    "    Returns:\n",
    "      The distance in meters.\n",
    "    \"\"\"\n",
    "    return 1000 * miles / MILES_TO_KM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ee1d04ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_combination_names(group):\n",
    "    \"\"\"Constructs a string representing combinations based on True values in a pandas GroupBy.\n",
    "\n",
    "    Generates a string by joining column names where the corresponding value in the Series is True.\n",
    "\n",
    "    This function is designed to be used with the `apply` method of a pandas GroupBy object.\n",
    "\n",
    "    Args:\n",
    "    group: A Series (from a pandas GroupBy) containing boolean values.\n",
    "\n",
    "    Returns:\n",
    "    A string with '-' joining column names that have True values, or an empty string if none.\n",
    "    \"\"\"\n",
    "\n",
    "    combination_names = []\n",
    "    for col_name, value in group.items():\n",
    "        if value.any():\n",
    "            # if there are any true values - keep the corresponding column name\n",
    "            combination_names.append(col_name)\n",
    "    return '-'.join(combination_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "255746e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_combination_names_hierarchy(group):\n",
    "    \"\"\"Determines a hierarchical name for a group in a pandas groupby based on True values.\n",
    "\n",
    "    Prioritizes the first column with all True values (out of four boolean transit stop buffer categories) \n",
    "    excluding 'No_Fixed_Route_Transit'.\n",
    "\n",
    "    This function is designed to be used with the `apply` method of a pandas GroupBy object.\n",
    "\n",
    "    Args:\n",
    "    group: A Series (from a pandas GroupBy) containing boolean values for the four transit stop buffer categories.\n",
    "\n",
    "    Returns:\n",
    "    A string representing the hierarchical name, either a feature name with '_only' \n",
    "    suffix, 'none' if only 'No_Fixed_Route_Transit' is True, or the original group \n",
    "    if no single feature has all True values.\n",
    "    \"\"\"\n",
    "    \n",
    "    # We loop through values in the tuple of booleans for each group - ordering matters. So\n",
    "    # we stop at the first case of a True value - the \n",
    "    # first one takes primacy - like major stops buffer\n",
    "    for col, val in group.items():\n",
    "        # print(col,val)\n",
    "        if val.all():\n",
    "            return f'{col}_only' if col!='No_Fixed_Route_Transit' else 'none'\n",
    "\n",
    "    return group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4a4c46d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def station_buffer(source_df, buf_dist):\n",
    "    \"\"\"Buffers a GeoDataFrame with a specified distance in miles.\n",
    "\n",
    "    Args:\n",
    "    source_df: A GeoDataFrame containing the stations to be buffered.\n",
    "    buf_dist: The distance in miles for the buffer zone.\n",
    "\n",
    "    Returns:\n",
    "    A GeoDataFrame representing the stations with a buffer applied \n",
    "    and dissolved to remove duplicate geometries.\n",
    "\n",
    "    Logs:\n",
    "    The size of the dataframe and the buffer applied.\n",
    "    \"\"\"\n",
    "    logger.info(f'\\tBuffering a df of {len(source_df)} records with {buf_dist:.02f} miles')\n",
    "    source_buf = source_df.buffer(miles_to_m(buf_dist))\n",
    "\n",
    "    output = (gpd.GeoDataFrame(  # data=source_subset[passthrough_cols],\n",
    "        geometry=source_buf.values)\n",
    "        .reset_index()\n",
    "        .rename(columns={'index': 'rowid'}))\n",
    "\n",
    "    return output.dissolve()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481d042a",
   "metadata": {},
   "source": [
    "# Create buffer for transit layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302bc965",
   "metadata": {},
   "source": [
    "## Set up logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4bd324f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create logger\n",
    "NOW = datetime.datetime.now()\n",
    "YDM = f'process_{NOW:%Y%m%d}'\n",
    "LOG_FILE = Path(WORKING_DIR, \"proximity2transit_{}.log\".format(NOW))\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel('DEBUG')\n",
    "\n",
    "# console handler\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel('INFO')\n",
    "ch.setFormatter(logging.Formatter(\n",
    "    '%(asctime)s - %(levelname)s - %(message)s', datefmt='%m/%d/%Y %I:%M:%S %p'))\n",
    "logger.addHandler(ch)\n",
    "\n",
    "# file handler\n",
    "fh = logging.FileHandler(LOG_FILE, mode='w')\n",
    "fh.setLevel('DEBUG')\n",
    "fh.setFormatter(logging.Formatter(\n",
    "    '%(asctime)s - %(levelname)s - %(message)s', datefmt='%m/%d/%Y %I:%M:%S %p'))\n",
    "logger.addHandler(fh)\n",
    "today_path = Path(WORKING_DIR,'outputs',YDM)\n",
    "today_path.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a9bf724e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_fgdb = Path(today_path,'file_geodatabase.gdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a75cbc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#shutil.copytree(Path(WORKING_DIR,'empty_fgdb.gdb'),new_fgdb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb335da3",
   "metadata": {},
   "source": [
    "## Step 0: Subset transit data to relevant universes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "27f1a08d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40237, 18)\n",
      "(40577, 18)\n",
      "(40232, 18)\n"
     ]
    }
   ],
   "source": [
    "# we use status criteria OR major_stop here -\n",
    "# following logic here: https://github.com/BayAreaMetro/petrale/blob/c4b96a98b291ada58e065375beccd5bf11e2da1b/scripts/proximity2transit.py#L128\n",
    "\n",
    "# subset to existing, under construction, open or FBP\n",
    "# this is really just everything\n",
    "\n",
    "# | major_stop==1\n",
    "qry_fbp = 'status.isin([\"Under Construction\",\"Open\",\"Final Blueprint\",\"Existing/Built\"]) '\n",
    "\n",
    "transit_fbp = transit_file.query(\n",
    "    qry_fbp)\n",
    "\n",
    "# subset to existing major stop, and under construction or open\n",
    "# | (major_stop==1 & status!=\"Final Blueprint\")\n",
    "qry_np = 'status.isin([\"Under Construction\",\"Open\",\"Existing/Built\"]) '\n",
    "\n",
    "transit_np = transit_file.query(\n",
    "    qry_np)\n",
    "\n",
    "# subset to existing transit infrastructure (per 2015 or so, pre-plan)\n",
    "\n",
    "qry_current = ' status==\"Existing/Built\"'\n",
    "#qry_current = 'major_stop==1 '\n",
    "transit_current = transit_file.query(\n",
    "    qry_current)\n",
    "\n",
    "print(transit_np.shape)\n",
    "print(transit_fbp.shape)\n",
    "print(transit_current.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a912d27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_filtering(input_data, filter_criteria):\n",
    "    \"\"\"Filters a DataFrame of transit stops based on specified criteria.\n",
    "\n",
    "    Args:\n",
    "        input_data (pd.DataFrame): The DataFrame containing input data.\n",
    "        filter_criteria (dict): A dictionary where keys are column names and values are tuples\n",
    "            containing the comparison operator and the value to compare against.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The filtered DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    key_numerics = ['am_av_hdwy', 'pm_av_hdwy', 'major_stop']\n",
    "\n",
    "    logger.info(\n",
    "        '\\tStarting length of the dataframe: {:,}'.format(len(input_data)))\n",
    "\n",
    "    # for filter_criteria in list_filter_criteria:\n",
    "    filter_components = []\n",
    "    for key, operator_values in filter_criteria.items():\n",
    "        filter_sub_components = []\n",
    "\n",
    "        # skip non-criteria related keys in dict\n",
    "        if not key in ['name', 'buffer']:\n",
    "            # print(operator_values)\n",
    "            for operator_value in operator_values:\n",
    "                value = operator_value['value']\n",
    "                operator = operator_value['operator']\n",
    "\n",
    "                # Generate query expression list from components\n",
    "\n",
    "                # if value is a string  - add quotes - otherwise assume to be numeric\n",
    "                lst_conditions = [\n",
    "                    f'{key}{operator}\"{value}\"' if isinstance(\n",
    "                        value, str) else f\"{key}{operator}{value}\"\n",
    "                ]\n",
    "\n",
    "                # Generate query string from query list\n",
    "                str_conditions = \"\".join(lst_conditions)\n",
    "                filter_sub_components.append(str_conditions)\n",
    "\n",
    "            # we are treating inner components as AND - so am_headway >10 and am_headway_15\n",
    "            # but we are treating AM and PM headways as OR - a stop with 12 minutes am and 25 minutes pm\n",
    "            # will be selected with the shorter headway\n",
    "            str_partial_query = '({})'.format(\n",
    "                \" & \".join(filter_sub_components))\n",
    "            filter_components.append(str_partial_query)\n",
    "\n",
    "    # Concatenate component filters\n",
    "    str_full_query = \" | \".join(filter_components)\n",
    "    logger.info(f'\\tQuery string: {str_full_query}')\n",
    "\n",
    "    # Create mask of matching records from component filters - to drop\n",
    "    keep_mask = input_data.eval(str_full_query)\n",
    "\n",
    "    if len(keep_mask.value_counts()) > 1:\n",
    "        keep_summary = input_data[keep_mask][key_numerics].median()\n",
    "        series_string = f\"\\tRecords Filtered: {'; '.join([f'{key}: {value:,.0f}' for key, value in keep_summary.items()])}\"\n",
    "\n",
    "#         logger.info(\n",
    "#             f'Applying filter: `{str_full_query}`\\n',\n",
    "#             f'\\Selecting {keep_mask.value_counts()[True]} records from dataframe\\n',\n",
    "#             #f'\\tRecords lost:\\n\\t{input_data[keep_mask][key_numerics].sum()}',\n",
    "#             series_string)\n",
    "\n",
    "        # applying to source data\n",
    "        input_data = input_data[keep_mask]\n",
    "        logger.info(f'\\tRecords after subsetting: {len(input_data):,.0f}')\n",
    "\n",
    "    else:\n",
    "        logger.info('Filters match no records. Keeping all records.')\n",
    "\n",
    "    # Return resulting df\n",
    "    return input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5ec004f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transit_filtered = data_filtering(transit_current, filter_criteria_hdwy_11_to_15)\n",
    "# transit_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "17d25fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "transit_buffers = {}\n",
    "\n",
    "def create_multiple_transit_areas(transit_stop_gdf, criteria_list, slug='current', write_to_disk=False):\n",
    "\n",
    "    logger.info(\n",
    "        f\"***Running create_transit_areas function for variant: {slug}***\")\n",
    "    logger.info(\n",
    "        f\"Transit Stop input geodataframe has shape: {transit_stop_gdf.shape}\")\n",
    "\n",
    "    # store filtered and buffered data\n",
    "    buffered_stops = {}\n",
    "\n",
    "    # store the names of the passed filter criteria - order matters to the hierarchy\n",
    "    # and needs to be known to the user - that major stops will trump 15 min headways, will trum 30 min headways etc\n",
    "    source_frames_ordered = []\n",
    "\n",
    "    logger.info('***Looping through criteria list to subset transit stops***')\n",
    "\n",
    "    # filter transit data, and in the same go, buffer the stops\n",
    "\n",
    "    for filter_criteria in criteria_list:\n",
    "        logger.info(f'Transit Station Subset: {filter_criteria[\"name\"]}')\n",
    "\n",
    "        # store filter condition name\n",
    "        source_frames_ordered.append(filter_criteria[\"name\"])\n",
    "        transit_filtered = data_filtering(transit_stop_gdf, filter_criteria)\n",
    "\n",
    "        # apply the filter\n",
    "        transit_filtered_buffered = station_buffer(\n",
    "            transit_filtered, filter_criteria['buffer'])\n",
    "\n",
    "        # store filtered and buffered data in a dict\n",
    "        buffered_stops[filter_criteria['name']] = transit_filtered_buffered\n",
    "\n",
    "        transit_filtered.to_file(Path(today_path,'transit_filter_{}.gpkg'.format(filter_criteria[\"name\"])),\n",
    "                                 driver='GeoJSON',engine='pyogrio')\n",
    "\n",
    "    # Also add the region as a final separate layer to union with, for wall-to-wall coverage\n",
    "    # TODO: find a more generalized approach for this than topping off with a regional layer\n",
    "    # of course could just omit the regional layer entirely and fillna on the parcel side\n",
    "    buffered_stops['No_Fixed_Route_Transit'] = region_bdry\n",
    "\n",
    "    # store the name as well\n",
    "    source_frames_ordered.append('No_Fixed_Route_Transit')\n",
    "\n",
    "    logger.info(\n",
    "        '***Union the transit stop buffers to identify transit service areas***')\n",
    "\n",
    "    #major_buf_name = 'Major_Transit_Stop'\n",
    "    major_buf_name = source_frames_ordered[0]\n",
    "\n",
    "    # From those stop buffers, build up the unions in a loop, starting with major stops\n",
    "    prior_frame = buffered_stops[major_buf_name].rename(\n",
    "        columns={'rowid': major_buf_name})\n",
    "\n",
    "    i = 1\n",
    "    for nme, current_frame in buffered_stops.items():\n",
    "\n",
    "        if nme != major_buf_name:\n",
    "            logger.info(\n",
    "                f'\\tUnion {i}: {\"->\".join(list(buffered_stops)[:i])}->{nme}')\n",
    "\n",
    "            # in each loop we get a new prior_frame which is just the union\n",
    "            # of past unions, and union with the current_frame\n",
    "            prior_frame = gpd.overlay(prior_frame,\n",
    "                                      current_frame.rename(\n",
    "                                          columns={'rowid': nme}),\n",
    "                                      how='union', keep_geom_type=True)\n",
    "            # this is a topology hack to avoid linestring\n",
    "            # TopologyException: found non-noded intersection between LINESTRING\n",
    "            prior_frame['geometry'] = prior_frame['geometry'].buffer(0.01)\n",
    "            i += 1\n",
    "\n",
    "#     logger.info(\n",
    "#         f'Result: a unioned frame of {len(prior_frame)} geographies')\n",
    "\n",
    "    # in this frame, a missing value for a column means that a certain area is outside the\n",
    "    # current column's transit buffer.\n",
    "    prior_frame = (prior_frame\n",
    "                   .fillna(-1)\n",
    "                   .replace({0: True, -1: False}))\n",
    "\n",
    "    # get a unique id for the combinations of presence / absence of different buffer types\n",
    "\n",
    "    prior_frame['combination_id'] = prior_frame.groupby(\n",
    "        source_frames_ordered).ngroup()\n",
    "\n",
    "    # Then, for each combination of overlapping areas, get the most important\n",
    "    # where rail (major stop) trumps shorter headways, which in turn trumps longer headways etc\n",
    "\n",
    "    cat_count = len(source_frames_ordered)\n",
    "    cat_count_var = f'cat{cat_count}'\n",
    "    \n",
    "    prior_frame[cat_count_var] = (prior_frame\n",
    "                           .combination_id\n",
    "                           .map(prior_frame\n",
    "                                .groupby(['combination_id'])[source_frames_ordered]\n",
    "                                .apply(\n",
    "                                    get_combination_names_hierarchy))\n",
    "                           )\n",
    "\n",
    "    # Get the full string list of components in any given unioned area - might be in the vicinity\n",
    "    # of a major stop, 15 min headway, 30 min headway, or just one or none of them. Which?\n",
    "\n",
    "    prior_frame['combination_id_desc'] = (prior_frame\n",
    "                                          .combination_id\n",
    "                                          .map(prior_frame\n",
    "                                               .groupby(['combination_id'])[source_frames_ordered]\n",
    "                                               .apply(\n",
    "                                                   get_combination_names))\n",
    "                                          )\n",
    "\n",
    "    # dissolve to just the core service category areas\n",
    "    prior_frame_dissolved = prior_frame.dissolve([cat_count_var],\n",
    "                                                 as_index=False)\n",
    "\n",
    "    logger.info(\n",
    "        f'Result: a unioned frame of {len(prior_frame)} component geographies, dissolved to {len(prior_frame_dissolved)}')\n",
    "\n",
    "    if write_to_disk:\n",
    "        logger.info(\n",
    "            f'Writing gdf of transit service level areas to disk for {slug}')\n",
    "        \n",
    "        out_path = Path(today_path,f'transit_service_levels_{slug}.geojson')\n",
    "        prior_frame_dissolved.to_file(\n",
    "            out_path, driver='GeoJSON',engine='pyogrio')\n",
    "        \n",
    "        out_path = Path(today_path,f'transit_service_levels_{slug}.gpkg')\n",
    "#         prior_frame_dissolved.to_file(\n",
    "#             out_path, \n",
    "#             driver='GPKG',engine='pyogrio')\n",
    "\n",
    "\n",
    "    # lastly, store a copy in this dict in the global namespace\n",
    "    transit_buffers[slug] = prior_frame_dissolved\n",
    "    return prior_frame, prior_frame_dissolved"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de570fa",
   "metadata": {},
   "source": [
    "## Step 1a: define filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8a419075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set criteria for selecting headways and what kind of buffer to apply to selected stops\n",
    "\n",
    "filter_criteria_major_stop = {'name': 'Major_Transit_Stop', 'buffer': .5,\n",
    "                              'major_stop': [{'operator': '==', 'value': 1}]\n",
    "                              }\n",
    "\n",
    "filter_criteria_hdwy_lt15 = {'name': 'Bus_<15min', 'buffer': .25,\n",
    "                             'am_av_hdwy': [{'operator': '<=', 'value': 15}],\n",
    "                             'pm_av_hdwy': [{'operator': '<=', 'value': 15}]}\n",
    "\n",
    "\n",
    "filter_criteria_hdwy_15_to_30 = {'name': 'Bus_15_30min', 'buffer': .25,\n",
    "                                 'am_av_hdwy': [{'operator': '>', 'value': 15}, {'operator': '<=', 'value': 30}],\n",
    "                                 'pm_av_hdwy': [{'operator': '>', 'value': 15}, {'operator': '<=', 'value': 30}]\n",
    "                                 }\n",
    "\n",
    "filter_criteria_hdwy_gt30 = {'name': 'Bus_30plusmin', 'buffer': .25,\n",
    "                             'am_av_hdwy': [{'operator': '>', 'value': 30}],\n",
    "                             'pm_av_hdwy': [{'operator': '>', 'value': 30}]}\n",
    "\n",
    "criteria_list = [filter_criteria_major_stop,\n",
    "                 filter_criteria_hdwy_lt15,\n",
    "                 filter_criteria_hdwy_15_to_30,\n",
    "                 filter_criteria_hdwy_gt30\n",
    "                 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5bfc38f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set criteria for selecting headways and what kind of buffer to apply to selected stops\n",
    "# ANUPs request\n",
    "\n",
    "filter_criteria_major_stop = {'name': 'Major_Transit_Stop', 'buffer': .5,\n",
    "                              'major_stop': [{'operator': '==', 'value': 1}]\n",
    "                              }\n",
    "\n",
    "filter_criteria_hdwy_lt10 = {'name': 'Bus_<10min', 'buffer': .25,\n",
    "                             'am_av_hdwy': [{'operator': '<=', 'value': 10}],\n",
    "                             'pm_av_hdwy': [{'operator': '<=', 'value': 10}]}\n",
    "\n",
    "\n",
    "filter_criteria_hdwy_11_to_15 = {'name': 'Bus_11_15min', 'buffer': .25,\n",
    "                                 'am_av_hdwy': [{'operator': '>', 'value': 10}, {'operator': '<=', 'value': 15}],\n",
    "                                 'pm_av_hdwy': [{'operator': '>', 'value': 10}, {'operator': '<=', 'value': 15}]\n",
    "                                 }\n",
    "filter_criteria_hdwy_15_to_30 = {'name': 'Bus_15_30min', 'buffer': .25,\n",
    "                                 'am_av_hdwy': [{'operator': '>', 'value': 15}, {'operator': '<=', 'value': 30}],\n",
    "                                 'pm_av_hdwy': [{'operator': '>', 'value': 15}, {'operator': '<=', 'value': 30}]\n",
    "                                 }\n",
    "\n",
    "filter_criteria_hdwy_gt30 = {'name': 'Bus_30plusmin', 'buffer': .25,\n",
    "                             'am_av_hdwy': [{'operator': '>', 'value': 30}],\n",
    "                             'pm_av_hdwy': [{'operator': '>', 'value': 30}]}\n",
    "\n",
    "criteria_list_anup = [filter_criteria_major_stop,\n",
    "                      filter_criteria_hdwy_lt10,\n",
    "                      filter_criteria_hdwy_11_to_15,\n",
    "                      filter_criteria_hdwy_15_to_30,\n",
    "                      filter_criteria_hdwy_gt30\n",
    "                      ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "4b4d5bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set criteria for selecting headways and what kind of buffer to apply to selected stops\n",
    "# ANUPs request\n",
    "\n",
    "\n",
    "filter_criteria_hdwy_lt10 = {'name': 'FREQUENT TRANSIT', 'buffer': .5,\n",
    "                             'am_av_hdwy': [{'operator': '<=', 'value': 10}],\n",
    "                             'pm_av_hdwy': [{'operator': '<=', 'value': 10}]}\n",
    "\n",
    "\n",
    "\n",
    "criteria_list_anup2 = [\n",
    "                      filter_criteria_hdwy_lt10\n",
    "                      ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c125a77",
   "metadata": {},
   "source": [
    "## Step 1: create transit buffers\n",
    "\n",
    "### PBA50 5-way categorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aca61a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/22/2024 01:19:40 PM - INFO - ***Running create_transit_areas function for variant: current_cat5***\n",
      "03/22/2024 01:19:40 PM - INFO - Transit Stop input geodataframe has shape: (40232, 18)\n",
      "03/22/2024 01:19:40 PM - INFO - ***Looping through criteria list to subset transit stops***\n",
      "03/22/2024 01:19:40 PM - INFO - Transit Station Subset: Major_Transit_Stop\n",
      "03/22/2024 01:19:40 PM - INFO - \tStarting length of the dataframe: 40,232\n",
      "03/22/2024 01:19:40 PM - INFO - \tQuery string: (major_stop==1)\n",
      "03/22/2024 01:19:40 PM - INFO - \tRecords after subsetting: 4,802\n",
      "03/22/2024 01:19:40 PM - INFO - \tBuffering a df of 4802 records with 0.50 miles\n",
      "03/22/2024 01:19:43 PM - INFO - Transit Station Subset: Bus_<15min\n",
      "03/22/2024 01:19:43 PM - INFO - \tStarting length of the dataframe: 40,232\n",
      "03/22/2024 01:19:43 PM - INFO - \tQuery string: (am_av_hdwy<=15) | (pm_av_hdwy<=15)\n",
      "03/22/2024 01:19:43 PM - INFO - \tRecords after subsetting: 15,832\n",
      "03/22/2024 01:19:43 PM - INFO - \tBuffering a df of 15832 records with 0.25 miles\n",
      "03/22/2024 01:19:48 PM - INFO - Transit Station Subset: Bus_15_30min\n",
      "03/22/2024 01:19:48 PM - INFO - \tStarting length of the dataframe: 40,232\n",
      "03/22/2024 01:19:48 PM - INFO - \tQuery string: (am_av_hdwy>15 & am_av_hdwy<=30) | (pm_av_hdwy>15 & pm_av_hdwy<=30)\n",
      "03/22/2024 01:19:48 PM - INFO - \tRecords after subsetting: 13,678\n",
      "03/22/2024 01:19:48 PM - INFO - \tBuffering a df of 13678 records with 0.25 miles\n",
      "03/22/2024 01:19:53 PM - INFO - Transit Station Subset: Bus_30plusmin\n",
      "03/22/2024 01:19:53 PM - INFO - \tStarting length of the dataframe: 40,232\n",
      "03/22/2024 01:19:53 PM - INFO - \tQuery string: (am_av_hdwy>30) | (pm_av_hdwy>30)\n",
      "03/22/2024 01:19:53 PM - INFO - \tRecords after subsetting: 11,586\n",
      "03/22/2024 01:19:53 PM - INFO - \tBuffering a df of 11586 records with 0.25 miles\n",
      "03/22/2024 01:19:57 PM - INFO - ***Union the transit stop buffers to identify transit service areas***\n",
      "03/22/2024 01:19:57 PM - INFO - \tUnion 1: Major_Transit_Stop->Bus_<15min\n",
      "03/22/2024 01:19:57 PM - INFO - \tUnion 2: Major_Transit_Stop->Bus_<15min->Bus_15_30min\n",
      "03/22/2024 01:19:58 PM - INFO - \tUnion 3: Major_Transit_Stop->Bus_<15min->Bus_15_30min->Bus_30plusmin\n",
      "03/22/2024 01:20:03 PM - INFO - \tUnion 4: Major_Transit_Stop->Bus_<15min->Bus_15_30min->Bus_30plusmin->No_Fixed_Route_Transit\n",
      "03/22/2024 01:20:23 PM - INFO - Result: a unioned frame of 17 component geographies, dissolved to 5\n",
      "03/22/2024 01:20:23 PM - INFO - Writing gdf of transit service level areas to disk for current_cat5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 44.9 s, sys: 1.22 s, total: 46.1 s\n",
      "Wall time: 1min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "transit_areas_current_cat5, transit_areas_current_dissolved_cat5 = create_multiple_transit_areas(\n",
    "    transit_current,criteria_list, 'current_cat5',True)\n",
    "\n",
    "transit_areas_fbp_cat5, transit_areas_fbp_dissolved_cat5 = create_multiple_transit_areas(\n",
    "    transit_fbp,criteria_list, 'fbp_cat5',True)\n",
    "\n",
    "\n",
    "transit_areas_np_cat5, transit_areas_np_dissolved_cat5 = create_multiple_transit_areas(\n",
    "    transit_np,criteria_list, 'np_cat5',True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df61c4c",
   "metadata": {},
   "source": [
    "### PBA50Plus 6 way categorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "34de38f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/22/2024 01:25:52 PM - INFO - ***Running create_transit_areas function for variant: current_cat6***\n",
      "03/22/2024 01:25:52 PM - INFO - Transit Stop input geodataframe has shape: (40232, 18)\n",
      "03/22/2024 01:25:52 PM - INFO - ***Looping through criteria list to subset transit stops***\n",
      "03/22/2024 01:25:52 PM - INFO - Transit Station Subset: Major_Transit_Stop\n",
      "03/22/2024 01:25:52 PM - INFO - \tStarting length of the dataframe: 40,232\n",
      "03/22/2024 01:25:52 PM - INFO - \tQuery string: (major_stop==1)\n",
      "03/22/2024 01:25:52 PM - INFO - \tRecords after subsetting: 4,802\n",
      "03/22/2024 01:25:52 PM - INFO - \tBuffering a df of 4802 records with 0.50 miles\n",
      "03/22/2024 01:25:54 PM - INFO - Transit Station Subset: Bus_<10min\n",
      "03/22/2024 01:25:54 PM - INFO - \tStarting length of the dataframe: 40,232\n",
      "03/22/2024 01:25:54 PM - INFO - \tQuery string: (am_av_hdwy<=10) | (pm_av_hdwy<=10)\n",
      "03/22/2024 01:25:54 PM - INFO - \tRecords after subsetting: 10,673\n",
      "03/22/2024 01:25:54 PM - INFO - \tBuffering a df of 10673 records with 0.25 miles\n",
      "03/22/2024 01:26:05 PM - INFO - Transit Station Subset: Bus_11_15min\n",
      "03/22/2024 01:26:05 PM - INFO - \tStarting length of the dataframe: 40,232\n",
      "03/22/2024 01:26:05 PM - INFO - \tQuery string: (am_av_hdwy>10 & am_av_hdwy<=15) | (pm_av_hdwy>10 & pm_av_hdwy<=15)\n",
      "03/22/2024 01:26:05 PM - INFO - \tRecords after subsetting: 8,278\n",
      "03/22/2024 01:26:05 PM - INFO - \tBuffering a df of 8278 records with 0.25 miles\n",
      "03/22/2024 01:26:08 PM - INFO - Transit Station Subset: Bus_15_30min\n",
      "03/22/2024 01:26:08 PM - INFO - \tStarting length of the dataframe: 40,232\n",
      "03/22/2024 01:26:08 PM - INFO - \tQuery string: (am_av_hdwy>15 & am_av_hdwy<=30) | (pm_av_hdwy>15 & pm_av_hdwy<=30)\n",
      "03/22/2024 01:26:08 PM - INFO - \tRecords after subsetting: 13,678\n",
      "03/22/2024 01:26:08 PM - INFO - \tBuffering a df of 13678 records with 0.25 miles\n",
      "03/22/2024 01:26:22 PM - INFO - Transit Station Subset: Bus_30plusmin\n",
      "03/22/2024 01:26:22 PM - INFO - \tStarting length of the dataframe: 40,232\n",
      "03/22/2024 01:26:22 PM - INFO - \tQuery string: (am_av_hdwy>30) | (pm_av_hdwy>30)\n",
      "03/22/2024 01:26:22 PM - INFO - \tRecords after subsetting: 11,586\n",
      "03/22/2024 01:26:22 PM - INFO - \tBuffering a df of 11586 records with 0.25 miles\n",
      "03/22/2024 01:26:27 PM - INFO - ***Union the transit stop buffers to identify transit service areas***\n",
      "03/22/2024 01:26:27 PM - INFO - \tUnion 1: Major_Transit_Stop->Bus_<10min\n",
      "03/22/2024 01:26:27 PM - INFO - \tUnion 2: Major_Transit_Stop->Bus_<10min->Bus_11_15min\n",
      "03/22/2024 01:26:28 PM - INFO - \tUnion 3: Major_Transit_Stop->Bus_<10min->Bus_11_15min->Bus_15_30min\n",
      "03/22/2024 01:26:32 PM - INFO - \tUnion 4: Major_Transit_Stop->Bus_<10min->Bus_11_15min->Bus_15_30min->Bus_30plusmin\n",
      "03/22/2024 01:26:41 PM - INFO - \tUnion 5: Major_Transit_Stop->Bus_<10min->Bus_11_15min->Bus_15_30min->Bus_30plusmin->No_Fixed_Route_Transit\n",
      "03/22/2024 01:27:18 PM - INFO - Result: a unioned frame of 33 component geographies, dissolved to 6\n",
      "03/22/2024 01:27:18 PM - INFO - Writing gdf of transit service level areas to disk for current_cat6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 13s, sys: 1.59 s, total: 1min 14s\n",
      "Wall time: 2min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "transit_areas_current_cat6, transit_areas_current_dissolved_cat6 = create_multiple_transit_areas(\n",
    "    transit_current,criteria_list_anup, 'current_cat6',True)\n",
    "\n",
    "transit_areas_fbp_cat6, transit_areas_fbp_dissolved_cat6 = create_multiple_transit_areas(\n",
    "    transit_fbp,criteria_list_anup, 'fbp_cat6',True)\n",
    "\n",
    "transit_areas_np_cat6, transit_areas_np_dissolved_cat6 = create_multiple_transit_areas(\n",
    "    transit_np,criteria_list_anup, 'np_cat6',True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce8badf",
   "metadata": {},
   "source": [
    "### PBA50Plus 1 way categorization (major transit service)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "ab51c9ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/22/2024 04:49:55 PM - INFO - ***Running create_transit_areas function for variant: current_cat2***\n",
      "03/22/2024 04:49:55 PM - INFO - Transit Stop input geodataframe has shape: (40232, 18)\n",
      "03/22/2024 04:49:55 PM - INFO - ***Looping through criteria list to subset transit stops***\n",
      "03/22/2024 04:49:55 PM - INFO - Transit Station Subset: FREQUENT TRANSIT\n",
      "03/22/2024 04:49:55 PM - INFO - \tStarting length of the dataframe: 40,232\n",
      "03/22/2024 04:49:55 PM - INFO - \tQuery string: (am_av_hdwy<=10) | (pm_av_hdwy<=10)\n",
      "03/22/2024 04:49:55 PM - INFO - \tRecords after subsetting: 10,673\n",
      "03/22/2024 04:49:55 PM - INFO - \tBuffering a df of 10673 records with 0.50 miles\n",
      "03/22/2024 04:49:58 PM - INFO - ***Union the transit stop buffers to identify transit service areas***\n",
      "03/22/2024 04:49:58 PM - INFO - \tUnion 1: FREQUENT TRANSIT->No_Fixed_Route_Transit\n",
      "03/22/2024 04:49:59 PM - INFO - Result: a unioned frame of 2 component geographies, dissolved to 2\n",
      "03/22/2024 04:49:59 PM - INFO - Writing gdf of transit service level areas to disk for current_cat2\n",
      "03/22/2024 04:50:00 PM - INFO - ***Running create_transit_areas function for variant: fbp_cat2***\n",
      "03/22/2024 04:50:00 PM - INFO - Transit Stop input geodataframe has shape: (40577, 18)\n",
      "03/22/2024 04:50:00 PM - INFO - ***Looping through criteria list to subset transit stops***\n",
      "03/22/2024 04:50:00 PM - INFO - Transit Station Subset: FREQUENT TRANSIT\n",
      "03/22/2024 04:50:00 PM - INFO - \tStarting length of the dataframe: 40,577\n",
      "03/22/2024 04:50:00 PM - INFO - \tQuery string: (am_av_hdwy<=10) | (pm_av_hdwy<=10)\n",
      "03/22/2024 04:50:00 PM - INFO - \tRecords after subsetting: 10,673\n",
      "03/22/2024 04:50:00 PM - INFO - \tBuffering a df of 10673 records with 0.50 miles\n",
      "03/22/2024 04:50:12 PM - INFO - ***Union the transit stop buffers to identify transit service areas***\n",
      "03/22/2024 04:50:12 PM - INFO - \tUnion 1: FREQUENT TRANSIT->No_Fixed_Route_Transit\n",
      "03/22/2024 04:50:12 PM - INFO - Result: a unioned frame of 2 component geographies, dissolved to 2\n",
      "03/22/2024 04:50:12 PM - INFO - Writing gdf of transit service level areas to disk for fbp_cat2\n",
      "03/22/2024 04:50:14 PM - INFO - ***Running create_transit_areas function for variant: np_cat2***\n",
      "03/22/2024 04:50:14 PM - INFO - Transit Stop input geodataframe has shape: (40237, 18)\n",
      "03/22/2024 04:50:14 PM - INFO - ***Looping through criteria list to subset transit stops***\n",
      "03/22/2024 04:50:14 PM - INFO - Transit Station Subset: FREQUENT TRANSIT\n",
      "03/22/2024 04:50:14 PM - INFO - \tStarting length of the dataframe: 40,237\n",
      "03/22/2024 04:50:14 PM - INFO - \tQuery string: (am_av_hdwy<=10) | (pm_av_hdwy<=10)\n",
      "03/22/2024 04:50:14 PM - INFO - \tRecords after subsetting: 10,673\n",
      "03/22/2024 04:50:14 PM - INFO - \tBuffering a df of 10673 records with 0.50 miles\n",
      "03/22/2024 04:50:16 PM - INFO - ***Union the transit stop buffers to identify transit service areas***\n",
      "03/22/2024 04:50:16 PM - INFO - \tUnion 1: FREQUENT TRANSIT->No_Fixed_Route_Transit\n",
      "03/22/2024 04:50:17 PM - INFO - Result: a unioned frame of 2 component geographies, dissolved to 2\n",
      "03/22/2024 04:50:17 PM - INFO - Writing gdf of transit service level areas to disk for np_cat2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.98 s, sys: 124 ms, total: 9.11 s\n",
      "Wall time: 22.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "transit_areas_current_cat2, transit_areas_current_dissolved_cat2 = create_multiple_transit_areas(\n",
    "    transit_current,criteria_list_anup2, 'current_cat2',True)\n",
    "\n",
    "transit_areas_fbp_cat2, transit_areas_fbp_dissolved_cat2 = create_multiple_transit_areas(\n",
    "    transit_fbp,criteria_list_anup2, 'fbp_cat2',True)\n",
    "\n",
    "\n",
    "transit_areas_np_cat2, transit_areas_np_dissolved_cat2 = create_multiple_transit_areas(\n",
    "    transit_np,criteria_list_anup2, 'np_cat2',True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c10a5e",
   "metadata": {},
   "source": [
    "### Inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cf71880a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mine_vs_bobbys_fbp = gpd.overlay(v2020_fbp_classes.to_crs(ANALYSIS_CRS),transit_areas_fbp_dissolved_cat5)\n",
    "\n",
    "# mine_vs_bobbys_fbp['area'] = mine_vs_bobbys_fbp.area\n",
    "\n",
    "# def pct(x): return x/x.sum()\n",
    "\n",
    "# mine_vs_bobbys = mine_vs_bobbys_fbp.groupby(['Service_Level', 'cat5']).area.sum(\n",
    "# ).groupby(level=0, observed=True).apply(pct).round(2).unstack()\n",
    "\n",
    "# # While the diagonal is clearly there, there are important differences. Why?\n",
    "# # Turns out there are data source differences between the 2000 version and the one on AGOL now. Unclear why.\n",
    "# sns.heatmap(mine_vs_bobbys, cmap='coolwarm', annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1ddf10c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing a gdal driver on my box\n",
    "# transit_areas_np_dissolved_cat6.to_file(new_fgdb, layer=\"transit_areas_np_dissolved_cat6\", driver=\"FileGDB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03b1fcf",
   "metadata": {},
   "source": [
    "## Step 2 - relate transit proximities to parcels (before adding any run specific data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "5222a028",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_transit_service_areas_to_parcels(parcels_geo, transit_areas, slug='current', variable='cat5'):\n",
    "\n",
    "    logger.info(f'Joining transit areas ({slug}) to parcels')\n",
    "\n",
    "    # core assignment, per sjoin\n",
    "    p10_x_transit_areas = gpd.sjoin(\n",
    "        parcels_geo, transit_areas, predicate='within')\n",
    "\n",
    "    # check for unassigned\n",
    "    unassigned_parcel_ids = set(\n",
    "        parcels_geo.PARCEL_ID)-set(p10_x_transit_areas.PARCEL_ID)\n",
    "\n",
    "    logger.info(\n",
    "        f'There were {len(unassigned_parcel_ids)} parcels without an assignment to a transit area.\\nWe assign to nearest area.')\n",
    "\n",
    "    p10_x_transit_areas_remainder = gpd.sjoin_nearest(parcels_geo.query(\n",
    "        'PARCEL_ID.isin(@unassigned_parcel_ids)'), transit_areas)\n",
    "\n",
    "    # combine the two\n",
    "    combo_assignments = pd.concat(\n",
    "        [p10_x_transit_areas, p10_x_transit_areas_remainder])\n",
    "\n",
    "    if (combo_assignments.PARCEL_ID.value_counts().sort_values() > 1).any():\n",
    "\n",
    "        # if there are parcel duplicates - get the fist classification\n",
    "        mapping = combo_assignments.groupby(\n",
    "            'PARCEL_ID')[variable].first()\n",
    "    else:\n",
    "        # return the original mapping\n",
    "\n",
    "        mapping = combo_assignments.set_index('PARCEL_ID')[variable]\n",
    "\n",
    "    return mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3bcb4f",
   "metadata": {},
   "source": [
    "### Assign service areas\n",
    "#### cat5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "050c458f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/22/2024 01:36:01 PM - INFO - Joining transit areas (np) to parcels\n",
      "03/22/2024 01:36:11 PM - INFO - There were 72 parcels without an assignment to a transit area.\n",
      "We assign to nearest area.\n",
      "03/22/2024 01:36:13 PM - INFO - Joining transit areas (fbp) to parcels\n",
      "03/22/2024 01:36:21 PM - INFO - There were 72 parcels without an assignment to a transit area.\n",
      "We assign to nearest area.\n",
      "03/22/2024 01:36:24 PM - INFO - Joining transit areas (current) to parcels\n",
      "03/22/2024 01:36:32 PM - INFO - There were 72 parcels without an assignment to a transit area.\n",
      "We assign to nearest area.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 30.9 s, sys: 2.02 s, total: 33 s\n",
      "Wall time: 33.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "p10_x_transit_area_np_cat5 = assign_transit_service_areas_to_parcels(parcels_geo=p10_topofix_pt, transit_areas=transit_areas_np_dissolved_cat5,slug= 'np', variable='cat5')\n",
    "p10_x_transit_area_fbp_cat5 = assign_transit_service_areas_to_parcels(p10_topofix_pt, transit_areas=transit_areas_fbp_dissolved_cat5, slug='fbp', variable='cat5')\n",
    "p10_x_transit_area_current_cat5 = assign_transit_service_areas_to_parcels(p10_topofix_pt, transit_areas=transit_areas_current_dissolved_cat5, slug='current', variable='cat5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cb023a",
   "metadata": {},
   "source": [
    "#### cat6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e49980eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/22/2024 01:37:38 PM - INFO - Joining transit areas (np) to parcels\n",
      "03/22/2024 01:37:48 PM - INFO - There were 72 parcels without an assignment to a transit area.\n",
      "We assign to nearest area.\n",
      "03/22/2024 01:37:50 PM - INFO - Joining transit areas (fbp) to parcels\n",
      "03/22/2024 01:37:59 PM - INFO - There were 72 parcels without an assignment to a transit area.\n",
      "We assign to nearest area.\n",
      "03/22/2024 01:38:02 PM - INFO - Joining transit areas (current) to parcels\n",
      "03/22/2024 01:38:11 PM - INFO - There were 72 parcels without an assignment to a transit area.\n",
      "We assign to nearest area.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 33.7 s, sys: 1.9 s, total: 35.6 s\n",
      "Wall time: 35.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "p10_x_transit_area_np_cat6 = assign_transit_service_areas_to_parcels(parcels_geo=p10_topofix_pt, transit_areas=transit_areas_np_dissolved_cat6, slug='np', variable='cat6')\n",
    "p10_x_transit_area_fbp_cat6 = assign_transit_service_areas_to_parcels(parcels_geo=p10_topofix_pt, transit_areas=transit_areas_fbp_dissolved_cat6,slug= 'fbp', variable='cat6')\n",
    "p10_x_transit_area_current_cat6 = assign_transit_service_areas_to_parcels(parcels_geo=p10_topofix_pt, transit_areas=transit_areas_current_dissolved_cat6, slug='current', variable='cat6')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb189e4",
   "metadata": {},
   "source": [
    "#### cat2 frequent transit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "56ddab88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/22/2024 04:50:19 PM - INFO - Joining transit areas (np) to parcels\n",
      "03/22/2024 04:50:28 PM - INFO - There were 72 parcels without an assignment to a transit area.\n",
      "We assign to nearest area.\n",
      "03/22/2024 04:50:30 PM - INFO - Joining transit areas (fbp) to parcels\n",
      "03/22/2024 04:50:39 PM - INFO - There were 72 parcels without an assignment to a transit area.\n",
      "We assign to nearest area.\n",
      "03/22/2024 04:50:41 PM - INFO - Joining transit areas (current) to parcels\n",
      "03/22/2024 04:50:50 PM - INFO - There were 72 parcels without an assignment to a transit area.\n",
      "We assign to nearest area.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29.5 s, sys: 2.81 s, total: 32.3 s\n",
      "Wall time: 32.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "p10_x_transit_area_np_cat2 = assign_transit_service_areas_to_parcels(parcels_geo=p10_topofix_pt, transit_areas=transit_areas_np_dissolved_cat1, slug='np', variable='cat2')\n",
    "p10_x_transit_area_fbp_cat2 = assign_transit_service_areas_to_parcels(parcels_geo=p10_topofix_pt, transit_areas=transit_areas_fbp_dissolved_cat1,slug= 'fbp', variable='cat2')\n",
    "p10_x_transit_area_current_cat2 = assign_transit_service_areas_to_parcels(parcels_geo=p10_topofix_pt, transit_areas=transit_areas_current_dissolved_cat1, slug='current', variable='cat2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1fe711",
   "metadata": {},
   "source": [
    "### Collect in parcel - x - service areas in dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "48c12be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the parcels-to-transit service level correspondences in a dict for easy retrieval\n",
    "\n",
    "transit_scenario_crosswalk = {'cur_cat5': p10_x_transit_area_current_cat5,\n",
    "                              'fbp_cat5': p10_x_transit_area_fbp_cat5,\n",
    "                              'np_cat5': p10_x_transit_area_np_cat5,\n",
    "                              'cur_cat6': p10_x_transit_area_current_cat6,\n",
    "                              'fbp_cat6': p10_x_transit_area_fbp_cat6,\n",
    "                              'np_cat6': p10_x_transit_area_np_cat6,\n",
    "                              'cur_cat2': p10_x_transit_area_current_cat2,\n",
    "                              'fbp_cat2': p10_x_transit_area_fbp_cat2,\n",
    "                              'np_cat2': p10_x_transit_area_np_cat2,\n",
    "                             }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc3d6e8",
   "metadata": {},
   "source": [
    "## Step 3 Classify topo parcels to transit service areas and tracts and epc based on parcel_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "709eb7bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/22/2024 04:51:04 PM - INFO - tract10 records with no value assigned: 1\n",
      "03/22/2024 04:51:07 PM - INFO - tract20 records with no value assigned: 1\n",
      "03/22/2024 04:51:07 PM - INFO - taz records with no value assigned: 0\n",
      "03/22/2024 04:51:07 PM - INFO - area_type records with no value assigned: 0\n",
      "03/22/2024 04:51:08 PM - INFO - epc21 records with no value assigned: 18\n",
      "03/22/2024 04:51:08 PM - INFO - is_epc21 records with no value assigned: 0\n",
      "03/22/2024 04:51:09 PM - INFO - epc24 records with no value assigned: 14\n",
      "03/22/2024 04:51:09 PM - INFO - is_epc24 records with no value assigned: 0\n",
      "03/22/2024 04:51:10 PM - INFO - hra23 records with no value assigned: 153139\n",
      "03/22/2024 04:51:10 PM - INFO - is_hra23 records with no value assigned: 0\n",
      "03/22/2024 04:51:10 PM - INFO - hra19 records with no value assigned: 153139\n",
      "03/22/2024 04:51:11 PM - INFO - is_hra19 records with no value assigned: 0\n",
      "03/22/2024 04:51:11 PM - INFO - Service_Level_cur_cat5 records with no value assigned: 1\n",
      "03/22/2024 04:51:11 PM - INFO - Service_Level_fbp_cat5 records with no value assigned: 1\n",
      "03/22/2024 04:51:12 PM - INFO - Service_Level_np_cat5 records with no value assigned: 1\n",
      "03/22/2024 04:51:12 PM - INFO - Service_Level_cur_cat6 records with no value assigned: 1\n",
      "03/22/2024 04:51:12 PM - INFO - Service_Level_fbp_cat6 records with no value assigned: 1\n",
      "03/22/2024 04:51:13 PM - INFO - Service_Level_np_cat6 records with no value assigned: 1\n",
      "03/22/2024 04:51:13 PM - INFO - Service_Level_cur_cat2 records with no value assigned: 1\n",
      "03/22/2024 04:51:14 PM - INFO - Service_Level_fbp_cat2 records with no value assigned: 1\n",
      "03/22/2024 04:51:14 PM - INFO - Service_Level_np_cat2 records with no value assigned: 1\n"
     ]
    }
   ],
   "source": [
    "def add_classifications_to_p10_topo(p10_topofix_pt):\n",
    "\n",
    "    ####################################################\n",
    "    # add tractids, tazs to topo parcels\n",
    "\n",
    "    # vintage 2010\n",
    "    p10_topofix_pt['tract10'] = p10_topofix_pt.PARCEL_ID.map(\n",
    "        parcel_x_tracts.set_index('parcel_id').tract10.map(lambda x: f'{x:011.0f}'))\n",
    "    col_values(p10_topofix_pt['tract10'], 'tract10')\n",
    "\n",
    "    # vintage 2020\n",
    "    p10_topofix_pt['tract20'] = p10_topofix_pt.PARCEL_ID.map(\n",
    "        parcel_x_tracts.set_index('parcel_id').tract20.map(lambda x: f'{x:011.0f}'))\n",
    "    col_values(p10_topofix_pt['tract20'], 'tract20')\n",
    "\n",
    "    # add TAZs\n",
    "    p10_topofix_pt['taz'] = p10_topofix_pt.PARCEL_ID.map(\n",
    "        parcel_x_taz.set_index('PARCEL_ID').ZONE_ID)\n",
    "    col_values(p10_topofix_pt['taz'], 'taz')\n",
    "\n",
    "    ####################################################\n",
    "\n",
    "    # add area type based on the just assigned taz\n",
    "    p10_topofix_pt['area_type'] = p10_topofix_pt.taz.map(taz_x_areatype)\n",
    "    col_values(p10_topofix_pt['area_type'], 'area_type')\n",
    "\n",
    "    # add epc (RTP2021 version) to parcels using tract10\n",
    "    p10_topofix_pt['epc21'] = p10_topofix_pt.tract10.map(\n",
    "        tract_epc21_file.set_index('geoid10').epc_class.fillna('Not EPC'))\n",
    "    col_values(p10_topofix_pt['epc21'], 'epc21')\n",
    "\n",
    "    p10_topofix_pt['is_epc21'] = p10_topofix_pt.epc21 != 'Not EPC'\n",
    "    col_values(p10_topofix_pt['is_epc21'], 'is_epc21')\n",
    "\n",
    "    # add epc (RTP2025 version) to parcels using tract10\n",
    "    p10_topofix_pt['epc24'] = p10_topofix_pt.tract20.map(\n",
    "        tract_epc24_file.set_index('geoid20').epc_class.fillna('Not EPC'))\n",
    "    col_values(p10_topofix_pt['epc24'], 'epc24')\n",
    "\n",
    "    p10_topofix_pt['is_epc24'] = p10_topofix_pt.epc24 != 'Not EPC'\n",
    "    col_values(p10_topofix_pt['is_epc24'], 'is_epc24')\n",
    "\n",
    "    # add hra23 to parcels using tract10 (!)\n",
    "    p10_topofix_pt['hra23'] = p10_topofix_pt.tract10.map(\n",
    "        tract_hra_2023.set_index('geoid').hra_category)\n",
    "    col_values(p10_topofix_pt['hra23'], 'hra23')\n",
    "\n",
    "    p10_topofix_pt['is_hra23'] = p10_topofix_pt.hra23.isin(\n",
    "        [\"High Resource\", \"Highest Resource\"])\n",
    "    col_values(p10_topofix_pt['is_hra23'], 'is_hra23')\n",
    "\n",
    "    # add hra19 to parcels using tract10\n",
    "    p10_topofix_pt['hra19'] = p10_topofix_pt.tract10.map(\n",
    "        tract_hra_2019.set_index('geoid').hra_category)\n",
    "    col_values(p10_topofix_pt['hra19'], 'hra19')\n",
    "\n",
    "    p10_topofix_pt['is_hra19'] = p10_topofix_pt.hra19.isin(\n",
    "        [\"High Resource\", \"Highest Resource\"])\n",
    "    col_values(p10_topofix_pt['is_hra19'], 'is_hra19')\n",
    "\n",
    "#     # map the cat5 service level names to more descriptive ones used in Tableau\n",
    "#     p10_topofix_pt['Service_Level'] = p10_topofix_pt.PARCEL_ID.map(\n",
    "#         transit_scenario_crosswalk[transit_scenario])\n",
    "#     col_values(p10_topofix_pt['Service_Level'], 'Service_Level')\n",
    "\n",
    "    for key, val in transit_scenario_crosswalk.items():\n",
    "        service_level_var = f'Service_Level_{key}'\n",
    "\n",
    "        p10_topofix_pt[service_level_var] = p10_topofix_pt.PARCEL_ID.map(\n",
    "            transit_scenario_crosswalk[key])\n",
    "        col_values(p10_topofix_pt[service_level_var], service_level_var)\n",
    "\n",
    "    return p10_topofix_pt\n",
    "\n",
    "\n",
    "p10_topofix_pt2 = add_classifications_to_p10_topo(p10_topofix_pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "ea8ec5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "p10_topofix_pt2[p10_topofix_pt2.hra19.notna()].to_file('/Users/aolsen/Downloads/p10_topofix_pt2_Hra19missing.geojson',driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "64f9d1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # only 3 tracts in parcels are not in the tract file\n",
    "# #parcel to tract crosswalking is impeccable \n",
    "# set(p10_topofix_pt2.tract10)-set(tract_2010.GEOID10)\n",
    "\n",
    "# set(tract_2010.GEOID10) - set(p10_topofix_pt2.tract10)\n",
    "\n",
    "# set(p10_topofix_pt2.tract20)-set(tract_2020.GEOID)\n",
    "\n",
    "# set(tract_2020.GEOID) - set(p10_topofix_pt2.tract20)\n",
    "\n",
    "# # show tracts in parcels where hra19 couldn't be assigned\n",
    "# set(p10_topofix_pt2[p10_topofix_pt2.hra19.isna()].tract10) - \\\n",
    "#     set(tract_hra_2019.geoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "4a0d8068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set(p10_topofix_pt2[p10_topofix_pt2.hra19.isna()].tract10) - \\\n",
    "#     set(tract_2010.GEOID10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "36d7e0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tract_2010.query('GEOID10==\"06097150303\"').explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "95538151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tract_hra_2019.set_index('geoid').hra_category.loc['06081613501']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "c419c8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# p10_topofix_pt.tract10  # .map(tract_hra_2023.set_index('geoid').hra_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "06e5c694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.74 s, sys: 621 ms, total: 4.36 s\n",
      "Wall time: 1min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "p10_topofix_pt2.to_feather(Path(WORKING_DIR,'outputs',YDM,'p10_topofix_pt2.feather'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "49da34e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.55 s, sys: 355 ms, total: 4.91 s\n",
      "Wall time: 36.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "p10_topofix_pt2.to_parquet(Path(WORKING_DIR,'outputs',YDM,'p10_topofix_pt2.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d9fedf63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.77 s, sys: 632 ms, total: 3.41 s\n",
      "Wall time: 10.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "p10_topofix_pt3 = gpd.read_parquet(Path(WORKING_DIR,'outputs',YDM,'p10_topofix_pt2.parquet'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab57432",
   "metadata": {},
   "source": [
    "## Step 4: run the summaries\n",
    "Note that we want to do the proper matching:\n",
    "* 2015 should always be \"current\"\n",
    "* 2050 blueprint should be matched with 2050 blueprint transit buffers\n",
    "* 2050 no project should be matched with 2050 no project transit buffers\n",
    "\n",
    "We are looking for output as follows [metrics_proximity.csv](https://mtcdrive.app.box.com/file/837349294434)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "4b9f6bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_proximity_target_schema = pd.read_csv(\n",
    "    '/Users/aolsen/Box/Horizon and Plan Bay Area 2050/Equity and Performance/7_Analysis/Metrics/Metrics_Outputs_FinalBlueprint/Intermediate Metrics/Archive Draft Plan Mar 2021/metrics_proximity.csv')\n",
    "schema_iteration_vars = ['Service_Level', 'year',\n",
    "                         'modelrunID', 'transit', 'area', 'blueprint']\n",
    "# metrics_proximity_target_schema.groupby(schema_iteration_vars).size().unstack('modelrunID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "09dff4bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/22/2024 03:31:58 PM - INFO - trt records with no value assigned: 17\n"
     ]
    }
   ],
   "source": [
    "def col_values(s, colname):\n",
    "    #msg = f'values for {colname}\\n n{s.value_counts()}'\n",
    "    msg2 = f'{colname} records with no value assigned: {len(s[s.isna()])}'\n",
    "    # logger.info(msg)\n",
    "    logger.info(msg2)\n",
    "\n",
    "\n",
    "col_values(parcel_x_tracts.tract10, 'trt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "5d431b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_mapping_parcels = {\n",
    "    'PBA50Plus_DBP_InitialRun': {'run_path':  'PBA50Plus_DBP_InitialRun_v6',\n",
    "                                 'mnemonic': 'PBA50Plus_DBP_InitialRun_v6',\n",
    "                                 'root_path': '/Volumes/Data/Models/urban_modeling/baus/PBA50Plus/PBA50Plus_DBP_InitialRun/outputs'},\n",
    "\n",
    "    'PBA50Plus_NP_InitialRun': {'run_path':  'PBA50Plus_NP_InitialRun_v7',\n",
    "                                'mnemonic': 'PBA50Plus_NP_InitialRun_v7',\n",
    "                                'root_path': '/Volumes/Data/Models/urban_modeling/baus/PBA50Plus/PBA50Plus_NP_InitialRun/outputs'},\n",
    "    'PBA50_NP': {'run_path':  'EIR runs/Baseline Large (s25) runs/NP_v8_FINAL',\n",
    "                 'mnemonic': 'PBA50_NP',\n",
    "                 'run_name_stub': 'run314',\n",
    "                 'root_path': urbansim_run_location\n",
    "                 },\n",
    "    'PBA50_FBP': {'run_path':  'Final Blueprint runs/Final Blueprint (s24)/BAUS v2.25 - FINAL VERSION',\n",
    "                  'mnemonic': 'PBA50_FBP',\n",
    "                  'run_name_stub': 'run182',\n",
    "                  'root_path': urbansim_run_location\n",
    "                  },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "fa3b9a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "parcel_cols_keep = ['parcel_id', 'tothh',\n",
    "                    'hhq1', 'totemp', 'RETEMPN', 'MWTEMPN']\n",
    "\n",
    "\n",
    "def parcel_file_name(run_mnemonic, year):\n",
    "\n",
    "    if 'run_name_stub' in run_mapping_parcels[run_mnemonic]:\n",
    "        print(1)\n",
    "        parcel_template = '{}_parcel_data_{}.csv'.format(\n",
    "            run_mapping_parcels[run_mnemonic]['run_name_stub'], year)\n",
    "    else:\n",
    "        parcel_template = 'parcel_data_{}.csv'.format(year)\n",
    "    out_path = Path(run_mapping_parcels[run_mnemonic]['root_path'],\n",
    "                    run_mapping_parcels[run_mnemonic]['run_path'], parcel_template)\n",
    "    return out_path\n",
    "\n",
    "\n",
    "parcel_output = pd.read_csv(parcel_file_name(\n",
    "    'PBA50_FBP', 2050), usecols=parcel_cols_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "1d3cad22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# p10_topofix_pt_mrg = p10_topofix_pt2.merge(\n",
    "#     parcel_output, left_on=['PARCEL_ID'], right_on=['parcel_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "072d3bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# p10_topofix_pt_mrg.filter(regex='cat6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "ffc862ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_parcels(parcel_output, year, transit_scenario):\n",
    "    \"\"\"Summarizes parcel data by transit service levels and geographic groupings.\n",
    "\n",
    "    This function reads a DataFrame containing parcel information, prepares it for analysis,\n",
    "    and then calculates summaries based on transit service levels, Equity Priority\n",
    "    Communities (EPCs), High Resource Areas (HRAs), and area types.\n",
    "\n",
    "    Args:\n",
    "      parcel_output: A pandas DataFrame containing information about parcels.\n",
    "      year: The year for which the summaries are generated (likely used for filtering data).\n",
    "      transit_scenario: The transit scenario to focus on (e.g., 'fbp' for frequent bus service).\n",
    "\n",
    "    Returns:\n",
    "      A pandas DataFrame with summaries for various transit service levels, EPCs, HRAs, and area types.\n",
    "    \"\"\"\n",
    "\n",
    "    # Read or assume parcel data is already loaded in parcel_output\n",
    "    # logger.info(' Read {} rows'.format(len(parcel_output)))\n",
    "\n",
    "    # Drop extraneous cols\n",
    "    # parcel_output.drop(['geom_id', 'total_job_spaces', 'zoned_du',\n",
    "    #                    'zoned_du_underbuild', 'zoned_du_underbuild_nodev', 'first_building_type'], axis=1, inplace=True)\n",
    "\n",
    "    # Log informative message about data\n",
    "    # logger.info(\"Head:\\n{}\".format(parcel_output.head()))\n",
    "\n",
    "    # Define columns containing values of interest\n",
    "    val_cols = ['totemp', 'RETEMPN', 'MWTEMPN', 'tothh']\n",
    "\n",
    "    # Fill missing values and convert specific columns to integers for consistency\n",
    "    for col in val_cols:\n",
    "        parcel_output[col] = parcel_output[col].fillna(0).round(0).astype(int)\n",
    "\n",
    "    # Merge parcel run data geographic classifiers / categorizations\n",
    "    p10_topofix_pt_mrg = p10_topofix_pt2.merge(\n",
    "        parcel_output, left_on=['PARCEL_ID'], right_on=['parcel_id'])\n",
    "\n",
    "    #return p10_topofix_pt_mrg\n",
    "    def groupby_summaries(df, group_vars):\n",
    "\n",
    "        grp_summary = (df\n",
    "                       .groupby(group_vars)\n",
    "                       .agg({'tothh': 'sum', 'hhq1': 'sum',\n",
    "                             'totemp': 'sum', 'RETEMPN': 'sum',\n",
    "                             'MWTEMPN': 'sum'})\n",
    "                       )\n",
    "\n",
    "        grp_summary_shares = grp_summary.div(\n",
    "            grp_summary.sum()).round(3)\n",
    "\n",
    "        grp_summary_combo = pd.concat([grp_summary,\n",
    "                                       grp_summary_shares.add_suffix('_share')\n",
    "                                       ], axis=1)\n",
    "        return grp_summary_combo\n",
    "\n",
    "    # Identify the passed scenario-specific columns (fbp,no project, current)\n",
    "    # this returns different classifications for each - like the 5-way or 6-way service level (cat5, cat6) \n",
    "    # we summarize run data for each classification variable\n",
    "    transit_svcs_cols = p10_topofix_pt_mrg.filter(\n",
    "        regex=transit_scenario).columns\n",
    "\n",
    "    container = {}\n",
    "\n",
    "    # Calculate summaries for various groupings\n",
    "    for var in transit_svcs_cols:\n",
    "        summary_servicelevel = groupby_summaries(p10_topofix_pt_mrg, var)\n",
    "\n",
    "        summary_servicelevel_epc = groupby_summaries(\n",
    "            p10_topofix_pt_mrg, ['is_epc24', var])\n",
    "\n",
    "        summary_servicelevel_hra = groupby_summaries(\n",
    "            p10_topofix_pt_mrg, ['is_hra23', var])\n",
    "\n",
    "        summary_servicelevel_areatype = groupby_summaries(\n",
    "            p10_topofix_pt_mrg, ['area_type', var])\n",
    "\n",
    "        container[('Region',var)] = pd.concat([summary_servicelevel], keys=['reg'])\n",
    "        container[('CoCs',var)] = summary_servicelevel_epc\n",
    "        container[('HRAs',var)] = summary_servicelevel_hra\n",
    "        container[('area_type',var)] = summary_servicelevel_areatype\n",
    "    return pd.concat(container)\n",
    "\n",
    "\n",
    "parcel_output_summary_trn_fp_cat5 = summarize_parcels(\n",
    "    parcel_output, 2050, 'fbp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "c7a264e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>tothh</th>\n",
       "      <th>hhq1</th>\n",
       "      <th>totemp</th>\n",
       "      <th>RETEMPN</th>\n",
       "      <th>MWTEMPN</th>\n",
       "      <th>tothh_share</th>\n",
       "      <th>hhq1_share</th>\n",
       "      <th>totemp_share</th>\n",
       "      <th>RETEMPN_share</th>\n",
       "      <th>MWTEMPN_share</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Service_Level_fbp_cat5</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">reg</th>\n",
       "      <th>Bus_15_30min_only</th>\n",
       "      <td>474217</td>\n",
       "      <td>66392.0</td>\n",
       "      <td>838520</td>\n",
       "      <td>78434</td>\n",
       "      <td>123208</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.155</td>\n",
       "      <td>0.178</td>\n",
       "      <td>0.203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bus_30plusmin_only</th>\n",
       "      <td>229297</td>\n",
       "      <td>27901.0</td>\n",
       "      <td>316267</td>\n",
       "      <td>26679</td>\n",
       "      <td>41895</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bus_&lt;15min_only</th>\n",
       "      <td>565775</td>\n",
       "      <td>135653.0</td>\n",
       "      <td>674561</td>\n",
       "      <td>75155</td>\n",
       "      <td>73564</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Major_Transit_Stop_only</th>\n",
       "      <td>1970295</td>\n",
       "      <td>818833.0</td>\n",
       "      <td>2780290</td>\n",
       "      <td>221425</td>\n",
       "      <td>192785</td>\n",
       "      <td>0.488</td>\n",
       "      <td>0.717</td>\n",
       "      <td>0.514</td>\n",
       "      <td>0.501</td>\n",
       "      <td>0.317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>none</th>\n",
       "      <td>801973</td>\n",
       "      <td>93401.0</td>\n",
       "      <td>797175</td>\n",
       "      <td>39862</td>\n",
       "      <td>176197</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.147</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">Service_Level_fbp_cat6</th>\n",
       "      <th rowspan=\"6\" valign=\"top\">reg</th>\n",
       "      <th>Bus_11_15min_only</th>\n",
       "      <td>173820</td>\n",
       "      <td>42198.0</td>\n",
       "      <td>226703</td>\n",
       "      <td>18103</td>\n",
       "      <td>24937</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bus_15_30min_only</th>\n",
       "      <td>474217</td>\n",
       "      <td>66392.0</td>\n",
       "      <td>838393</td>\n",
       "      <td>78398</td>\n",
       "      <td>123206</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.155</td>\n",
       "      <td>0.178</td>\n",
       "      <td>0.203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bus_30plusmin_only</th>\n",
       "      <td>229297</td>\n",
       "      <td>27901.0</td>\n",
       "      <td>316267</td>\n",
       "      <td>26679</td>\n",
       "      <td>41895</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bus_&lt;10min_only</th>\n",
       "      <td>391960</td>\n",
       "      <td>93457.0</td>\n",
       "      <td>447858</td>\n",
       "      <td>57052</td>\n",
       "      <td>48627</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Major_Transit_Stop_only</th>\n",
       "      <td>1970291</td>\n",
       "      <td>818832.0</td>\n",
       "      <td>2780417</td>\n",
       "      <td>221461</td>\n",
       "      <td>192787</td>\n",
       "      <td>0.488</td>\n",
       "      <td>0.717</td>\n",
       "      <td>0.514</td>\n",
       "      <td>0.502</td>\n",
       "      <td>0.317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>none</th>\n",
       "      <td>801972</td>\n",
       "      <td>93400.0</td>\n",
       "      <td>797175</td>\n",
       "      <td>39862</td>\n",
       "      <td>176197</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.147</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Service_Level_fbp_cat2</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">reg</th>\n",
       "      <th>FREQUENT TRANSIT_only</th>\n",
       "      <td>2458809</td>\n",
       "      <td>913657.0</td>\n",
       "      <td>3329219</td>\n",
       "      <td>288934</td>\n",
       "      <td>271425</td>\n",
       "      <td>0.608</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.616</td>\n",
       "      <td>0.654</td>\n",
       "      <td>0.447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>none</th>\n",
       "      <td>1582748</td>\n",
       "      <td>228523.0</td>\n",
       "      <td>2077594</td>\n",
       "      <td>152621</td>\n",
       "      <td>336224</td>\n",
       "      <td>0.392</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.384</td>\n",
       "      <td>0.346</td>\n",
       "      <td>0.553</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      tothh      hhq1  \\\n",
       "Service_Level_fbp_cat5 reg Bus_15_30min_only         474217   66392.0   \n",
       "                           Bus_30plusmin_only        229297   27901.0   \n",
       "                           Bus_<15min_only           565775  135653.0   \n",
       "                           Major_Transit_Stop_only  1970295  818833.0   \n",
       "                           none                      801973   93401.0   \n",
       "Service_Level_fbp_cat6 reg Bus_11_15min_only         173820   42198.0   \n",
       "                           Bus_15_30min_only         474217   66392.0   \n",
       "                           Bus_30plusmin_only        229297   27901.0   \n",
       "                           Bus_<10min_only           391960   93457.0   \n",
       "                           Major_Transit_Stop_only  1970291  818832.0   \n",
       "                           none                      801972   93400.0   \n",
       "Service_Level_fbp_cat2 reg FREQUENT TRANSIT_only    2458809  913657.0   \n",
       "                           none                     1582748  228523.0   \n",
       "\n",
       "                                                     totemp  RETEMPN  MWTEMPN  \\\n",
       "Service_Level_fbp_cat5 reg Bus_15_30min_only         838520    78434   123208   \n",
       "                           Bus_30plusmin_only        316267    26679    41895   \n",
       "                           Bus_<15min_only           674561    75155    73564   \n",
       "                           Major_Transit_Stop_only  2780290   221425   192785   \n",
       "                           none                      797175    39862   176197   \n",
       "Service_Level_fbp_cat6 reg Bus_11_15min_only         226703    18103    24937   \n",
       "                           Bus_15_30min_only         838393    78398   123206   \n",
       "                           Bus_30plusmin_only        316267    26679    41895   \n",
       "                           Bus_<10min_only           447858    57052    48627   \n",
       "                           Major_Transit_Stop_only  2780417   221461   192787   \n",
       "                           none                      797175    39862   176197   \n",
       "Service_Level_fbp_cat2 reg FREQUENT TRANSIT_only    3329219   288934   271425   \n",
       "                           none                     2077594   152621   336224   \n",
       "\n",
       "                                                    tothh_share  hhq1_share  \\\n",
       "Service_Level_fbp_cat5 reg Bus_15_30min_only              0.117       0.058   \n",
       "                           Bus_30plusmin_only             0.057       0.024   \n",
       "                           Bus_<15min_only                0.140       0.119   \n",
       "                           Major_Transit_Stop_only        0.488       0.717   \n",
       "                           none                           0.198       0.082   \n",
       "Service_Level_fbp_cat6 reg Bus_11_15min_only              0.043       0.037   \n",
       "                           Bus_15_30min_only              0.117       0.058   \n",
       "                           Bus_30plusmin_only             0.057       0.024   \n",
       "                           Bus_<10min_only                0.097       0.082   \n",
       "                           Major_Transit_Stop_only        0.488       0.717   \n",
       "                           none                           0.198       0.082   \n",
       "Service_Level_fbp_cat2 reg FREQUENT TRANSIT_only          0.608       0.800   \n",
       "                           none                           0.392       0.200   \n",
       "\n",
       "                                                    totemp_share  \\\n",
       "Service_Level_fbp_cat5 reg Bus_15_30min_only               0.155   \n",
       "                           Bus_30plusmin_only              0.058   \n",
       "                           Bus_<15min_only                 0.125   \n",
       "                           Major_Transit_Stop_only         0.514   \n",
       "                           none                            0.147   \n",
       "Service_Level_fbp_cat6 reg Bus_11_15min_only               0.042   \n",
       "                           Bus_15_30min_only               0.155   \n",
       "                           Bus_30plusmin_only              0.058   \n",
       "                           Bus_<10min_only                 0.083   \n",
       "                           Major_Transit_Stop_only         0.514   \n",
       "                           none                            0.147   \n",
       "Service_Level_fbp_cat2 reg FREQUENT TRANSIT_only           0.616   \n",
       "                           none                            0.384   \n",
       "\n",
       "                                                    RETEMPN_share  \\\n",
       "Service_Level_fbp_cat5 reg Bus_15_30min_only                0.178   \n",
       "                           Bus_30plusmin_only               0.060   \n",
       "                           Bus_<15min_only                  0.170   \n",
       "                           Major_Transit_Stop_only          0.501   \n",
       "                           none                             0.090   \n",
       "Service_Level_fbp_cat6 reg Bus_11_15min_only                0.041   \n",
       "                           Bus_15_30min_only                0.178   \n",
       "                           Bus_30plusmin_only               0.060   \n",
       "                           Bus_<10min_only                  0.129   \n",
       "                           Major_Transit_Stop_only          0.502   \n",
       "                           none                             0.090   \n",
       "Service_Level_fbp_cat2 reg FREQUENT TRANSIT_only            0.654   \n",
       "                           none                             0.346   \n",
       "\n",
       "                                                    MWTEMPN_share  \n",
       "Service_Level_fbp_cat5 reg Bus_15_30min_only                0.203  \n",
       "                           Bus_30plusmin_only               0.069  \n",
       "                           Bus_<15min_only                  0.121  \n",
       "                           Major_Transit_Stop_only          0.317  \n",
       "                           none                             0.290  \n",
       "Service_Level_fbp_cat6 reg Bus_11_15min_only                0.041  \n",
       "                           Bus_15_30min_only                0.203  \n",
       "                           Bus_30plusmin_only               0.069  \n",
       "                           Bus_<10min_only                  0.080  \n",
       "                           Major_Transit_Stop_only          0.317  \n",
       "                           none                             0.290  \n",
       "Service_Level_fbp_cat2 reg FREQUENT TRANSIT_only            0.447  \n",
       "                           none                             0.553  "
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parcel_output_summary_trn_fp_cat5.loc['Region']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f1ffba",
   "metadata": {},
   "outputs": [],
   "source": [
    "parcel_output_summary_trn_fp_cat6 = summarize_parcels(\n",
    "    parcel_output, 2050, 'fbp_cat6')\n",
    "parcel_output_summary_trn_np_cat5 = summarize_parcels(\n",
    "    parcel_output, 2050, 'trn_np_cat5')\n",
    "parcel_output_summary_trn_np_cat6 = summarize_parcels(\n",
    "    parcel_output, 2050, 'trn_np_cat6')\n",
    "parcel_output_summary_trn_cur_cat5 = summarize_parcels(\n",
    "    parcel_output, 2050, 'trn_cur_cat5')\n",
    "parcel_output_summary_trn_cur_cat6 = summarize_parcels(\n",
    "    parcel_output, 2050, 'trn_cur_cat6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b4c6ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ccb69e49",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'parcel_output_summary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mparcel_output_summary\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'parcel_output_summary' is not defined"
     ]
    }
   ],
   "source": [
    "parcel_output_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "id": "b31e2654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copying in the code from the metrics script for reference - \n",
    "# it is mainly an accounting script in the sense that the outputs are really coming from Bobby's scripts\n",
    "# and by extension this notebook's scripts - share of households, etc by different transit service geographes\n",
    "\n",
    "def calculate_Connected1_proximity(runid, year, dbp, transitproximity_df, metrics_dict):\n",
    "\n",
    "    metric_id = \"C1\"\n",
    "\n",
    "    for area_type in ['Region', 'CoCs', 'HRAs', 'rural', 'suburban', 'urban']:\n",
    "        # households\n",
    "        metrics_dict[runid, metric_id, 'transitproximity_majorstop_shareof_tothh_%s' % area_type, year, dbp] = transitproximity_df.loc[(transitproximity_df['Service_Level'] == \"Major_Transit_Stop\")\n",
    "                                                                                                                                       & (transitproximity_df['year'] == int(year))\n",
    "                                                                                                                                       & (transitproximity_df['blueprint'].str.contains(dbp))\n",
    "                                                                                                                                       & (transitproximity_df['area'] == area_type), 'tothh_share'].sum()\n",
    "        metrics_dict[runid, metric_id, 'transitproximity_majorstop_shareof_hhq1_%s' % area_type, year, dbp] = transitproximity_df.loc[(transitproximity_df['Service_Level'] == \"Major_Transit_Stop\")\n",
    "                                                                                                                                      & (transitproximity_df['year'] == int(year))\n",
    "                                                                                                                                      & (transitproximity_df['blueprint'].str.contains(dbp))\n",
    "                                                                                                                                      & (transitproximity_df['area'] == area_type), 'hhq1_share'].sum()\n",
    "        # jobs\n",
    "        metrics_dict[runid, metric_id, 'transitproximity_majorstop_shareof_totemp_%s' % area_type, year, dbp] = transitproximity_df.loc[(transitproximity_df['Service_Level'] == \"Major_Transit_Stop\")\n",
    "                                                                                                                                        & (transitproximity_df['year'] == int(year))\n",
    "                                                                                                                                        & (transitproximity_df['blueprint'].str.contains(dbp))\n",
    "                                                                                                                                        & (transitproximity_df['area'] == area_type), 'totemp_share'].sum()\n",
    "        metrics_dict[runid, metric_id, 'transitproximity_majorstop_shareof_RETEMPNjobs_%s' % area_type, year, dbp] = transitproximity_df.loc[(transitproximity_df['Service_Level'] == \"Major_Transit_Stop\")\n",
    "                                                                                                                                             & (transitproximity_df['year'] == int(year))\n",
    "                                                                                                                                             & (transitproximity_df['blueprint'].str.contains(dbp))\n",
    "                                                                                                                                             & (transitproximity_df['area'] == area_type), 'RETEMPN_share'].sum()\n",
    "        metrics_dict[runid, metric_id, 'transitproximity_majorstop_shareof_MWTEMPNjobs_%s' % area_type, year, dbp] = transitproximity_df.loc[(transitproximity_df['Service_Level'] == \"Major_Transit_Stop\")\n",
    "                                                                                                                                             & (transitproximity_df['year'] == int(year)) & (transitproximity_df['blueprint'].str.contains(dbp))\n",
    "                                                                                                                                             & (transitproximity_df['area'] == area_type), 'MWTEMPN_share'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0788dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "32dd1faf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>modelrunID</th>\n",
       "      <th>metric</th>\n",
       "      <th>name</th>\n",
       "      <th>year</th>\n",
       "      <th>blueprint</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2640</th>\n",
       "      <td>2050_TM152_FBP_PlusCrossing_24</td>\n",
       "      <td>C1</td>\n",
       "      <td>transitproximity_majorstop_shareof_tothh_Region</td>\n",
       "      <td>2050</td>\n",
       "      <td>Plus</td>\n",
       "      <td>0.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2641</th>\n",
       "      <td>2050_TM152_FBP_PlusCrossing_24</td>\n",
       "      <td>C1</td>\n",
       "      <td>transitproximity_majorstop_shareof_hhq1_Region</td>\n",
       "      <td>2050</td>\n",
       "      <td>Plus</td>\n",
       "      <td>0.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2642</th>\n",
       "      <td>2050_TM152_FBP_PlusCrossing_24</td>\n",
       "      <td>C1</td>\n",
       "      <td>transitproximity_majorstop_shareof_totemp_Region</td>\n",
       "      <td>2050</td>\n",
       "      <td>Plus</td>\n",
       "      <td>0.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2643</th>\n",
       "      <td>2050_TM152_FBP_PlusCrossing_24</td>\n",
       "      <td>C1</td>\n",
       "      <td>transitproximity_majorstop_shareof_RETEMPNjobs...</td>\n",
       "      <td>2050</td>\n",
       "      <td>Plus</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2644</th>\n",
       "      <td>2050_TM152_FBP_PlusCrossing_24</td>\n",
       "      <td>C1</td>\n",
       "      <td>transitproximity_majorstop_shareof_MWTEMPNjobs...</td>\n",
       "      <td>2050</td>\n",
       "      <td>Plus</td>\n",
       "      <td>0.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2645</th>\n",
       "      <td>2050_TM152_FBP_PlusCrossing_24</td>\n",
       "      <td>C1</td>\n",
       "      <td>transitproximity_majorstop_shareof_tothh_CoCs</td>\n",
       "      <td>2050</td>\n",
       "      <td>Plus</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2646</th>\n",
       "      <td>2050_TM152_FBP_PlusCrossing_24</td>\n",
       "      <td>C1</td>\n",
       "      <td>transitproximity_majorstop_shareof_hhq1_CoCs</td>\n",
       "      <td>2050</td>\n",
       "      <td>Plus</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2647</th>\n",
       "      <td>2050_TM152_FBP_PlusCrossing_24</td>\n",
       "      <td>C1</td>\n",
       "      <td>transitproximity_majorstop_shareof_totemp_CoCs</td>\n",
       "      <td>2050</td>\n",
       "      <td>Plus</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2648</th>\n",
       "      <td>2050_TM152_FBP_PlusCrossing_24</td>\n",
       "      <td>C1</td>\n",
       "      <td>transitproximity_majorstop_shareof_RETEMPNjobs...</td>\n",
       "      <td>2050</td>\n",
       "      <td>Plus</td>\n",
       "      <td>0.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2649</th>\n",
       "      <td>2050_TM152_FBP_PlusCrossing_24</td>\n",
       "      <td>C1</td>\n",
       "      <td>transitproximity_majorstop_shareof_MWTEMPNjobs...</td>\n",
       "      <td>2050</td>\n",
       "      <td>Plus</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2650</th>\n",
       "      <td>2050_TM152_FBP_PlusCrossing_24</td>\n",
       "      <td>C1</td>\n",
       "      <td>transitproximity_majorstop_shareof_tothh_HRAs</td>\n",
       "      <td>2050</td>\n",
       "      <td>Plus</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2651</th>\n",
       "      <td>2050_TM152_FBP_PlusCrossing_24</td>\n",
       "      <td>C1</td>\n",
       "      <td>transitproximity_majorstop_shareof_hhq1_HRAs</td>\n",
       "      <td>2050</td>\n",
       "      <td>Plus</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2652</th>\n",
       "      <td>2050_TM152_FBP_PlusCrossing_24</td>\n",
       "      <td>C1</td>\n",
       "      <td>transitproximity_majorstop_shareof_totemp_HRAs</td>\n",
       "      <td>2050</td>\n",
       "      <td>Plus</td>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2653</th>\n",
       "      <td>2050_TM152_FBP_PlusCrossing_24</td>\n",
       "      <td>C1</td>\n",
       "      <td>transitproximity_majorstop_shareof_RETEMPNjobs...</td>\n",
       "      <td>2050</td>\n",
       "      <td>Plus</td>\n",
       "      <td>0.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2654</th>\n",
       "      <td>2050_TM152_FBP_PlusCrossing_24</td>\n",
       "      <td>C1</td>\n",
       "      <td>transitproximity_majorstop_shareof_MWTEMPNjobs...</td>\n",
       "      <td>2050</td>\n",
       "      <td>Plus</td>\n",
       "      <td>0.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2655</th>\n",
       "      <td>2050_TM152_FBP_PlusCrossing_24</td>\n",
       "      <td>C1</td>\n",
       "      <td>transitproximity_majorstop_shareof_tothh_rural</td>\n",
       "      <td>2050</td>\n",
       "      <td>Plus</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2656</th>\n",
       "      <td>2050_TM152_FBP_PlusCrossing_24</td>\n",
       "      <td>C1</td>\n",
       "      <td>transitproximity_majorstop_shareof_hhq1_rural</td>\n",
       "      <td>2050</td>\n",
       "      <td>Plus</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2657</th>\n",
       "      <td>2050_TM152_FBP_PlusCrossing_24</td>\n",
       "      <td>C1</td>\n",
       "      <td>transitproximity_majorstop_shareof_totemp_rural</td>\n",
       "      <td>2050</td>\n",
       "      <td>Plus</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2658</th>\n",
       "      <td>2050_TM152_FBP_PlusCrossing_24</td>\n",
       "      <td>C1</td>\n",
       "      <td>transitproximity_majorstop_shareof_RETEMPNjobs...</td>\n",
       "      <td>2050</td>\n",
       "      <td>Plus</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2659</th>\n",
       "      <td>2050_TM152_FBP_PlusCrossing_24</td>\n",
       "      <td>C1</td>\n",
       "      <td>transitproximity_majorstop_shareof_MWTEMPNjobs...</td>\n",
       "      <td>2050</td>\n",
       "      <td>Plus</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2660</th>\n",
       "      <td>2050_TM152_FBP_PlusCrossing_24</td>\n",
       "      <td>C1</td>\n",
       "      <td>transitproximity_majorstop_shareof_tothh_suburban</td>\n",
       "      <td>2050</td>\n",
       "      <td>Plus</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2661</th>\n",
       "      <td>2050_TM152_FBP_PlusCrossing_24</td>\n",
       "      <td>C1</td>\n",
       "      <td>transitproximity_majorstop_shareof_hhq1_suburban</td>\n",
       "      <td>2050</td>\n",
       "      <td>Plus</td>\n",
       "      <td>0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2662</th>\n",
       "      <td>2050_TM152_FBP_PlusCrossing_24</td>\n",
       "      <td>C1</td>\n",
       "      <td>transitproximity_majorstop_shareof_totemp_subu...</td>\n",
       "      <td>2050</td>\n",
       "      <td>Plus</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2663</th>\n",
       "      <td>2050_TM152_FBP_PlusCrossing_24</td>\n",
       "      <td>C1</td>\n",
       "      <td>transitproximity_majorstop_shareof_RETEMPNjobs...</td>\n",
       "      <td>2050</td>\n",
       "      <td>Plus</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2664</th>\n",
       "      <td>2050_TM152_FBP_PlusCrossing_24</td>\n",
       "      <td>C1</td>\n",
       "      <td>transitproximity_majorstop_shareof_MWTEMPNjobs...</td>\n",
       "      <td>2050</td>\n",
       "      <td>Plus</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2665</th>\n",
       "      <td>2050_TM152_FBP_PlusCrossing_24</td>\n",
       "      <td>C1</td>\n",
       "      <td>transitproximity_majorstop_shareof_tothh_urban</td>\n",
       "      <td>2050</td>\n",
       "      <td>Plus</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2666</th>\n",
       "      <td>2050_TM152_FBP_PlusCrossing_24</td>\n",
       "      <td>C1</td>\n",
       "      <td>transitproximity_majorstop_shareof_hhq1_urban</td>\n",
       "      <td>2050</td>\n",
       "      <td>Plus</td>\n",
       "      <td>0.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2667</th>\n",
       "      <td>2050_TM152_FBP_PlusCrossing_24</td>\n",
       "      <td>C1</td>\n",
       "      <td>transitproximity_majorstop_shareof_totemp_urban</td>\n",
       "      <td>2050</td>\n",
       "      <td>Plus</td>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2668</th>\n",
       "      <td>2050_TM152_FBP_PlusCrossing_24</td>\n",
       "      <td>C1</td>\n",
       "      <td>transitproximity_majorstop_shareof_RETEMPNjobs...</td>\n",
       "      <td>2050</td>\n",
       "      <td>Plus</td>\n",
       "      <td>0.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2669</th>\n",
       "      <td>2050_TM152_FBP_PlusCrossing_24</td>\n",
       "      <td>C1</td>\n",
       "      <td>transitproximity_majorstop_shareof_MWTEMPNjobs...</td>\n",
       "      <td>2050</td>\n",
       "      <td>Plus</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          modelrunID metric  \\\n",
       "2640  2050_TM152_FBP_PlusCrossing_24     C1   \n",
       "2641  2050_TM152_FBP_PlusCrossing_24     C1   \n",
       "2642  2050_TM152_FBP_PlusCrossing_24     C1   \n",
       "2643  2050_TM152_FBP_PlusCrossing_24     C1   \n",
       "2644  2050_TM152_FBP_PlusCrossing_24     C1   \n",
       "2645  2050_TM152_FBP_PlusCrossing_24     C1   \n",
       "2646  2050_TM152_FBP_PlusCrossing_24     C1   \n",
       "2647  2050_TM152_FBP_PlusCrossing_24     C1   \n",
       "2648  2050_TM152_FBP_PlusCrossing_24     C1   \n",
       "2649  2050_TM152_FBP_PlusCrossing_24     C1   \n",
       "2650  2050_TM152_FBP_PlusCrossing_24     C1   \n",
       "2651  2050_TM152_FBP_PlusCrossing_24     C1   \n",
       "2652  2050_TM152_FBP_PlusCrossing_24     C1   \n",
       "2653  2050_TM152_FBP_PlusCrossing_24     C1   \n",
       "2654  2050_TM152_FBP_PlusCrossing_24     C1   \n",
       "2655  2050_TM152_FBP_PlusCrossing_24     C1   \n",
       "2656  2050_TM152_FBP_PlusCrossing_24     C1   \n",
       "2657  2050_TM152_FBP_PlusCrossing_24     C1   \n",
       "2658  2050_TM152_FBP_PlusCrossing_24     C1   \n",
       "2659  2050_TM152_FBP_PlusCrossing_24     C1   \n",
       "2660  2050_TM152_FBP_PlusCrossing_24     C1   \n",
       "2661  2050_TM152_FBP_PlusCrossing_24     C1   \n",
       "2662  2050_TM152_FBP_PlusCrossing_24     C1   \n",
       "2663  2050_TM152_FBP_PlusCrossing_24     C1   \n",
       "2664  2050_TM152_FBP_PlusCrossing_24     C1   \n",
       "2665  2050_TM152_FBP_PlusCrossing_24     C1   \n",
       "2666  2050_TM152_FBP_PlusCrossing_24     C1   \n",
       "2667  2050_TM152_FBP_PlusCrossing_24     C1   \n",
       "2668  2050_TM152_FBP_PlusCrossing_24     C1   \n",
       "2669  2050_TM152_FBP_PlusCrossing_24     C1   \n",
       "\n",
       "                                                   name  year blueprint value  \n",
       "2640    transitproximity_majorstop_shareof_tothh_Region  2050      Plus  0.49  \n",
       "2641     transitproximity_majorstop_shareof_hhq1_Region  2050      Plus  0.74  \n",
       "2642   transitproximity_majorstop_shareof_totemp_Region  2050      Plus  0.51  \n",
       "2643  transitproximity_majorstop_shareof_RETEMPNjobs...  2050      Plus   0.5  \n",
       "2644  transitproximity_majorstop_shareof_MWTEMPNjobs...  2050      Plus  0.32  \n",
       "2645      transitproximity_majorstop_shareof_tothh_CoCs  2050      Plus  0.68  \n",
       "2646       transitproximity_majorstop_shareof_hhq1_CoCs  2050      Plus  0.82  \n",
       "2647     transitproximity_majorstop_shareof_totemp_CoCs  2050      Plus  0.66  \n",
       "2648  transitproximity_majorstop_shareof_RETEMPNjobs...  2050      Plus  0.61  \n",
       "2649  transitproximity_majorstop_shareof_MWTEMPNjobs...  2050      Plus   0.4  \n",
       "2650      transitproximity_majorstop_shareof_tothh_HRAs  2050      Plus  0.43  \n",
       "2651       transitproximity_majorstop_shareof_hhq1_HRAs  2050      Plus   0.7  \n",
       "2652     transitproximity_majorstop_shareof_totemp_HRAs  2050      Plus  0.46  \n",
       "2653  transitproximity_majorstop_shareof_RETEMPNjobs...  2050      Plus  0.48  \n",
       "2654  transitproximity_majorstop_shareof_MWTEMPNjobs...  2050      Plus  0.39  \n",
       "2655     transitproximity_majorstop_shareof_tothh_rural  2050      Plus     0  \n",
       "2656      transitproximity_majorstop_shareof_hhq1_rural  2050      Plus  0.01  \n",
       "2657    transitproximity_majorstop_shareof_totemp_rural  2050      Plus     0  \n",
       "2658  transitproximity_majorstop_shareof_RETEMPNjobs...  2050      Plus  0.01  \n",
       "2659  transitproximity_majorstop_shareof_MWTEMPNjobs...  2050      Plus     0  \n",
       "2660  transitproximity_majorstop_shareof_tothh_suburban  2050      Plus  0.07  \n",
       "2661   transitproximity_majorstop_shareof_hhq1_suburban  2050      Plus  0.11  \n",
       "2662  transitproximity_majorstop_shareof_totemp_subu...  2050      Plus  0.05  \n",
       "2663  transitproximity_majorstop_shareof_RETEMPNjobs...  2050      Plus  0.07  \n",
       "2664  transitproximity_majorstop_shareof_MWTEMPNjobs...  2050      Plus  0.05  \n",
       "2665     transitproximity_majorstop_shareof_tothh_urban  2050      Plus  0.41  \n",
       "2666      transitproximity_majorstop_shareof_hhq1_urban  2050      Plus  0.63  \n",
       "2667    transitproximity_majorstop_shareof_totemp_urban  2050      Plus  0.46  \n",
       "2668  transitproximity_majorstop_shareof_RETEMPNjobs...  2050      Plus  0.42  \n",
       "2669  transitproximity_majorstop_shareof_MWTEMPNjobs...  2050      Plus  0.26  "
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = pd.read_csv('/Users/aolsen/Box/Horizon and Plan Bay Area 2050/Equity and Performance/7_Analysis/Metrics/Metrics_Outputs_FinalBlueprint/metrics.csv')\n",
    "metrics.loc[(metrics.modelrunID.str.contains(\"PlusCrossing\"))&(metrics['name'].str.contains('transitprox'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "19ecb08d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>modelrunID</th>\n",
       "      <th>metric</th>\n",
       "      <th>name</th>\n",
       "      <th>year</th>\n",
       "      <th>blueprint</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>EIR runs/Baseline Large (s25) runs/NP_v8_FINAL...</td>\n",
       "      <td>A2</td>\n",
       "      <td>deed_restricted_total</td>\n",
       "      <td>2050</td>\n",
       "      <td>NoProject</td>\n",
       "      <td>574252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>EIR runs/Baseline Large (s25) runs/NP_v8_FINAL...</td>\n",
       "      <td>A2</td>\n",
       "      <td>deed_restricted_total</td>\n",
       "      <td>2015</td>\n",
       "      <td>NoProject</td>\n",
       "      <td>121292</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            modelrunID metric  \\\n",
       "168  EIR runs/Baseline Large (s25) runs/NP_v8_FINAL...     A2   \n",
       "169  EIR runs/Baseline Large (s25) runs/NP_v8_FINAL...     A2   \n",
       "\n",
       "                      name  year  blueprint   value  \n",
       "168  deed_restricted_total  2050  NoProject  574252  \n",
       "169  deed_restricted_total  2015  NoProject  121292  "
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.query('blueprint==\"NoProject\" & name==\"deed_restricted_total\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "a6ebeca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics[metrics.name.str.lower().str.contains('deed')].set_index(['modelrunID','year','blueprint','name']).value.unstack([0,2]).loc[2050].T.to_clipboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "42b9496b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Service_Level</th>\n",
       "      <th>tothh</th>\n",
       "      <th>hhq1</th>\n",
       "      <th>totemp</th>\n",
       "      <th>RETEMPN</th>\n",
       "      <th>MWTEMPN</th>\n",
       "      <th>tothh_share</th>\n",
       "      <th>hhq1_share</th>\n",
       "      <th>totemp_share</th>\n",
       "      <th>RETEMPN_share</th>\n",
       "      <th>MWTEMPN_share</th>\n",
       "      <th>year</th>\n",
       "      <th>modelrunID</th>\n",
       "      <th>transit</th>\n",
       "      <th>area</th>\n",
       "      <th>blueprint</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>Bus_15_30min</td>\n",
       "      <td>241445</td>\n",
       "      <td>30886</td>\n",
       "      <td>415937</td>\n",
       "      <td>36569</td>\n",
       "      <td>68977</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.11</td>\n",
       "      <td>2050</td>\n",
       "      <td>Final Blueprint runs/Final Blueprint (s24)/BAU...</td>\n",
       "      <td>trn_fp_cat5</td>\n",
       "      <td>Region</td>\n",
       "      <td>Plus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>Bus_31plus_min</td>\n",
       "      <td>281742</td>\n",
       "      <td>31131</td>\n",
       "      <td>379888</td>\n",
       "      <td>31432</td>\n",
       "      <td>60476</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.10</td>\n",
       "      <td>2050</td>\n",
       "      <td>Final Blueprint runs/Final Blueprint (s24)/BAU...</td>\n",
       "      <td>trn_fp_cat5</td>\n",
       "      <td>Region</td>\n",
       "      <td>Plus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>Bus_&lt;15min</td>\n",
       "      <td>786845</td>\n",
       "      <td>127837</td>\n",
       "      <td>1083359</td>\n",
       "      <td>115284</td>\n",
       "      <td>114261</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.19</td>\n",
       "      <td>2050</td>\n",
       "      <td>Final Blueprint runs/Final Blueprint (s24)/BAU...</td>\n",
       "      <td>trn_fp_cat5</td>\n",
       "      <td>Region</td>\n",
       "      <td>Plus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>Major_Transit_Stop</td>\n",
       "      <td>1971306</td>\n",
       "      <td>749222</td>\n",
       "      <td>2785232</td>\n",
       "      <td>221557</td>\n",
       "      <td>194184</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.32</td>\n",
       "      <td>2050</td>\n",
       "      <td>Final Blueprint runs/Final Blueprint (s24)/BAU...</td>\n",
       "      <td>trn_fp_cat5</td>\n",
       "      <td>Region</td>\n",
       "      <td>Plus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>No_Fixed_Route_Transit</td>\n",
       "      <td>761974</td>\n",
       "      <td>70889</td>\n",
       "      <td>744044</td>\n",
       "      <td>36717</td>\n",
       "      <td>169760</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2050</td>\n",
       "      <td>Final Blueprint runs/Final Blueprint (s24)/BAU...</td>\n",
       "      <td>trn_fp_cat5</td>\n",
       "      <td>Region</td>\n",
       "      <td>Plus</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Service_Level    tothh    hhq1   totemp  RETEMPN  MWTEMPN  \\\n",
       "165            Bus_15_30min   241445   30886   415937    36569    68977   \n",
       "166          Bus_31plus_min   281742   31131   379888    31432    60476   \n",
       "167              Bus_<15min   786845  127837  1083359   115284   114261   \n",
       "168      Major_Transit_Stop  1971306  749222  2785232   221557   194184   \n",
       "169  No_Fixed_Route_Transit   761974   70889   744044    36717   169760   \n",
       "\n",
       "     tothh_share  hhq1_share  totemp_share  RETEMPN_share  MWTEMPN_share  \\\n",
       "165         0.06        0.03          0.08           0.08           0.11   \n",
       "166         0.07        0.03          0.07           0.07           0.10   \n",
       "167         0.19        0.13          0.20           0.26           0.19   \n",
       "168         0.49        0.74          0.51           0.50           0.32   \n",
       "169         0.19        0.07          0.14           0.08           0.28   \n",
       "\n",
       "     year                                         modelrunID      transit  \\\n",
       "165  2050  Final Blueprint runs/Final Blueprint (s24)/BAU...  trn_fp_cat5   \n",
       "166  2050  Final Blueprint runs/Final Blueprint (s24)/BAU...  trn_fp_cat5   \n",
       "167  2050  Final Blueprint runs/Final Blueprint (s24)/BAU...  trn_fp_cat5   \n",
       "168  2050  Final Blueprint runs/Final Blueprint (s24)/BAU...  trn_fp_cat5   \n",
       "169  2050  Final Blueprint runs/Final Blueprint (s24)/BAU...  trn_fp_cat5   \n",
       "\n",
       "       area blueprint  \n",
       "165  Region      Plus  \n",
       "166  Region      Plus  \n",
       "167  Region      Plus  \n",
       "168  Region      Plus  \n",
       "169  Region      Plus  "
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_proximity_target_schema.query('blueprint==\"Plus\" & year==2050 & transit==\"trn_fp_cat5\" & area==\"Region\"') #& area==\"CoCs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "34739fe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['EIR runs/Baseline Large (s25) runs/NP_v8_FINAL/run314',\n",
       " 'EIR runs/Alt1 (s26) runs/Alt1_v3_test_far_tiers_FINAL_EIR_ALT/run375',\n",
       " 'EIR runs/Alt2 (s28) runs/Alt2_v1_FINAL_EIR_ALT/run374',\n",
       " 'Final Blueprint runs/Final Blueprint (s24)/BAUS v2.25 - FINAL VERSION/run182']"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urbansim_run_location = f'/Users/{user}/Box/Modeling and Surveys/Urban Modeling/Bay Area UrbanSim/PBA50/'\n",
    "# keep DBP for future\n",
    "us_2050_DBP_Final = 'Draft Blueprint runs/Blueprint Plus Crossing (s23)/v1.7.1- FINAL DRAFT BLUEPRINT/run98'\n",
    "us_2050_FBP_Final = 'Final Blueprint runs/Final Blueprint (s24)/BAUS v2.25 - FINAL VERSION/run182'\n",
    "us_2050_NP_EIR = 'EIR runs/Baseline Large (s25) runs/NP_v8_FINAL/run314'\n",
    "us_2050_ALT1_EIR = 'EIR runs/Alt1 (s26) runs/Alt1_v3_test_far_tiers_FINAL_EIR_ALT/run375'\n",
    "us_2050_ALT2_EIR = 'EIR runs/Alt2 (s28) runs/Alt2_v1_FINAL_EIR_ALT/run374'\n",
    "\n",
    "list_us_runid = [us_2050_NP_EIR, us_2050_ALT1_EIR, us_2050_ALT2_EIR,\n",
    "                 us_2050_FBP_Final]  # , us_2050_FBP_Final, us_2050_DBP_Final]\n",
    "list_us_runid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "2cb153d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/aolsen/Box/Modeling and Surveys/Urban Modeling/Bay Area UrbanSim/PBA50/'"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_mapping_parcels['PBA50_FBP']['root_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "3ff3d87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_year=2050\n",
    "us_2050_FBP_Final = 'EIR runs/Baseline Large (s25) runs/NP_v8_FINAL'\n",
    "run_id = 'run314'\n",
    "\n",
    "parcels_np50 = pd.read_csv(Path(urbansim_run_location,us_2050_FBP_Final,'{}_parcel_data_{}.csv'.format(run_id,model_year)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "7a4d8db3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4259618.0"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parcels_pba50.deed_restricted_units.sum()/parcels_pba50.residential_units.sum()\n",
    "parcels_pba50.residential_units.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "f239fcff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Final Blueprint runs/Final Blueprint (s24)/BAUS v2.25 - FINAL VERSION/run182'"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "us_2050_FBP_Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f04183f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_year=2050\n",
    "us_2050_FBP_Final = 'Final Blueprint runs/Final Blueprint (s24)/BAUS v2.25 - FINAL VERSION'\n",
    "run_id = 'run182'\n",
    "\n",
    "parcels_pba50 = pd.read_csv(Path(urbansim_run_location,us_2050_FBP_Final,'{}_parcel_data_{}_UBI.csv'.format(run_id,model_year)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e81a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pba_parcel_data(scenario):\n",
    "\n",
    "    column_map = {'tothh_share', 'hhq1_share',\n",
    "                  'totemp_share', 'RETEMPN_share', 'MWTEMPN_share'}\n",
    "\n",
    "    for model_year in (2015, 2050):\n",
    "        if model_year == 2050:\n",
    "            if us_runid == us_2050_FBP_Final:\n",
    "                parcel_file = urbansim_runid + \\\n",
    "                    '_parcel_data_{}_UBI.csv'.format(model_year)\n",
    "            else:\n",
    "                parcel_file = urbansim_runid + \\\n",
    "                    '_parcel_data_{}.csv'.format(model_year)\n",
    "        else:\n",
    "            parcel_file = urbansim_runid + \\\n",
    "                '_parcel_data_{}.csv'.format(model_year)\n",
    "        logger.info('Reading {} parcel data from {}'.format(\n",
    "            model_year, parcel_file))\n",
    "        parcel_output = pd.read_csv(parcel_file, engine='python')\n",
    "\n",
    "        logger.info('  Read {} rows'.format(len(parcel_output)))\n",
    "\n",
    "        # keep essential columns\n",
    "        parcel_output.drop(['geom_id', 'total_job_spaces', 'zoned_du',\n",
    "                            'zoned_du_underbuild', 'zoned_du_underbuild_nodev', 'first_building_type'], axis=1, inplace=True)\n",
    "        logger.info(\"Head:\\n{}\".format(parcel_output.head()))\n",
    "\n",
    "        parcel_output['totemp'] = parcel_output['totemp'].fillna(0)\n",
    "        parcel_output['totemp'] = parcel_output['totemp'].round(\n",
    "            0).astype('int')\n",
    "        parcel_output['RETEMPN'] = parcel_output['RETEMPN'].fillna(0)\n",
    "        parcel_output['RETEMPN'] = parcel_output['RETEMPN'].round(\n",
    "            0).astype('int')\n",
    "        parcel_output['MWTEMPN'] = parcel_output['MWTEMPN'].fillna(0)\n",
    "        parcel_output['MWTEMPN'] = parcel_output['MWTEMPN'].round(\n",
    "            0).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "8058f08e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tothh', 'hhq1', 'totemp', 'RETEMPN', 'MWTEMPN']"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec0036a",
   "metadata": {},
   "outputs": [],
   "source": [
    "['year','transit','modelrunID','tothh_share'  ,'hhq1_share'  ,'totemp_share' ,'RETEMPN_share','MWTEMPN_share','modelrunID']\n",
    "\n",
    "        prox_sdf_taz['tothh_share'  ] = round(prox_sdf_taz.tothh  / prox_sdf_taz.tothh.sum(),  2)\n",
    "        prox_sdf_taz['hhq1_share'   ] = round(prox_sdf_taz.hhq1   / prox_sdf_taz.hhq1.sum()  , 2)\n",
    "        prox_sdf_taz['totemp_share' ] = round(prox_sdf_taz.totemp / prox_sdf_taz.totemp.sum(), 2)\n",
    "        prox_sdf_taz['RETEMPN_share'] = round(prox_sdf_taz.RETEMPN/prox_sdf_taz.RETEMPN.sum(), 2)\n",
    "        prox_sdf_taz['MWTEMPN_share'] = round(prox_sdf_taz.MWTEMPN/prox_sdf_taz.MWTEMPN.sum(), 2)\n",
    "        prox_sdf_taz['year'] = str(model_year)\n",
    "        prox_sdf_taz['modelrunID'] = us_runid\n",
    "        prox_sdf_taz['transit'] = transit_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5092d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dae1544",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4786ed88",
   "metadata": {},
   "source": [
    "# Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "id": "40a2e1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multiple_transit_areas_old(transit_stop_gdf, scenario='current', write_to_disk=False):\n",
    "\n",
    "    logger.info(\n",
    "        f\"Running create_transit_areas function for variant: {scenario}\")\n",
    "    logger.info(f\"Transit Stop input geodataframe has shape: {transit_stop_gdf.shape}\")\n",
    "\n",
    "    buf_dist_half = .5\n",
    "\n",
    "    qry_str_major = \"(major_stop == 1)\"\n",
    "    logger.info(\n",
    "        f\"Subsetting stops to {qry_str_major}, and adding {buf_dist_half} mile buffer\")\n",
    "    major_buf = station_buffer(\n",
    "        transit_stop_gdf, qry_str_major, buf_dist_half)\n",
    "\n",
    "    buf_dist_qtr = .25\n",
    "\n",
    "    qry_str_hdwy_lt15 = \"(am_av_hdwy <= 15) & (pm_av_hdwy <= 15)\"\n",
    "    logger.info(\n",
    "        f\"Subsetting stops to {qry_str_hdwy_lt15}, and adding {buf_dist_qtr} mile buffer\")\n",
    "    hdwy15_buf = station_buffer(\n",
    "        transit_stop_gdf, qry_str_hdwy_lt15, buf_dist_qtr)\n",
    "\n",
    "    qry_str_hdwy_15_to_30 = \"( am_av_hdwy >15 ) & (am_av_hdwy <=30) & ( pm_av_hdwy >15 ) & (pm_av_hdwy <=30)  \"\n",
    "    logger.info(\n",
    "        f\"Subsetting stops to {qry_str_hdwy_15_to_30}, and adding {buf_dist_qtr} mile buffer\")\n",
    "    hdwy30_buf = station_buffer(\n",
    "        transit_stop_gdf, qry_str_hdwy_15_to_30, buf_dist_qtr)\n",
    "\n",
    "    qry_str_hdwy_gt30 = \"(am_av_hdwy > 30) | (pm_av_hdwy > 30)\"\n",
    "    logger.info(\n",
    "        f\"Subsetting stops to {qry_str_hdwy_gt30}, and adding {buf_dist_qtr} mile buffer\")\n",
    "    hdwy30plus_buf = station_buffer(\n",
    "        transit_stop_gdf, qry_str_hdwy_gt30, buf_dist_qtr)\n",
    "\n",
    "    ## Union and classify\n",
    "\n",
    "    # these are the frames we are interested in for the union. We will denote in the\n",
    "    # union process whether a geometry is covered by a particular spatial frame\n",
    "\n",
    "    # the ordering here matters - reflecting a hierarchy of interest: we will use\n",
    "    # to classify areas based on the highest order category in this list\n",
    "    source_frames_ordered = ['major_buf', 'hdwy15buf', 'hdwy30buf', 'hdwy30plusbuf',\n",
    "                             'reg']\n",
    "\n",
    "    # geopandas can only handle two input layers at a time when unioning. We do it step-wise.\n",
    "\n",
    "    logger.info(\n",
    "        f\"Beginning a series of unions, to get all buffers to fully define regional space\")\n",
    "\n",
    "    logger.info(f\"Unioning hdwy15_buf and hdwy30_buf\")\n",
    "\n",
    "    # row_ids are in the source frames; we disambiguate to denote whether a particular\n",
    "    # area is \"covered\" by either of the source files.\n",
    "    # As we are dealing with dissolved geometries for the source geometries, it means\n",
    "    # each source DF has just 1 row with multipart geoms - and that row_id then just\n",
    "    # has the index value of 0 carried forward. So in the resulting union, a row_id value\n",
    "    # of 0 means that a given area was represented in the relevant source dataframe;\n",
    "    # a value of NaN means there was no corresponding match for a particular dataframe\n",
    "    # this is useful for classifying areas based on the \"source\" buffer geometries present\n",
    "    # for a particular location\n",
    "\n",
    "    union_1 = gpd.overlay(hdwy15_buf, hdwy30_buf, how='union').reset_index().rename(\n",
    "        columns={'index': 'df_1', 'rowid_1': 'hdwy15buf', 'rowid_2': 'hdwy30buf'})\n",
    "\n",
    "    logger.info(f'union_1 shape: {union_1.shape}')\n",
    "\n",
    "    logger.info(f\"Adding hdwy30plus_buf...\")\n",
    "    union_2 = gpd.overlay(union_1, hdwy30plus_buf, how='union').reset_index().rename(\n",
    "        columns={'index': 'df_2', 'rowid': 'hdwy30plusbuf'})\n",
    "\n",
    "    # return union_1,hdwy15_buf, hdwy30_buf,union_2,hdwy30plus_buf\n",
    "\n",
    "    logger.info(f'union_2 shape: {union_2.shape}')\n",
    "\n",
    "    logger.info(f\"Adding major stops buffer...\")\n",
    "\n",
    "    union_3 = gpd.overlay(union_2, major_buf, how='union').reset_index(\n",
    "    ).rename(columns={'index': 'df_3', 'rowid': 'major_buf'})\n",
    "\n",
    "    logger.info(f'union_3 shape: {union_3.shape}')\n",
    "    logger.info(\n",
    "        f\"Adding regional boundary so we can get wall to wall coverage\")\n",
    "\n",
    "    # added small (here 10cm) geometry simplification to avoid a TopologyException: found non-noded intersection\n",
    "    union_3['geometry'] = union_3.simplify(.1)\n",
    "\n",
    "    union_4 = gpd.overlay(union_3, region_bdry, how='union').reset_index(\n",
    "    ).rename(columns={'index': 'df_4', 'name': 'reg'})\n",
    "    logger.info(f'union_4 shape: {union_4.shape}')\n",
    "    logger.info('-'*80)\n",
    "    logger.info(f'union_4 head: {union_4.head().T}')\n",
    "    logger.info('-'*80)\n",
    "\n",
    "    # except:\n",
    "#     logger.error(\"Traceback info:\\n{}\\nError Info:\\n{}\".format(\n",
    "#         'unions', str(sys.exc_info()[1])))\n",
    "\n",
    "    # fill nas with -1 - those are values where a particular transit buffer layer doesn't cover something\n",
    "    union_4 = union_4.fillna(-1)\n",
    "\n",
    "    # we need boolean values to grab the group names\n",
    "    logger.info(\n",
    "        f\"With the full region unioned, we now focus on classifying the different combinations\")\n",
    "    union_4[source_frames_ordered] = union_4[source_frames_ordered].replace(\n",
    "        {0: True, -1: False})\n",
    "\n",
    "    # get an integer encoding the unique combination of union components.\n",
    "    # e.g. some areas will be in the buffer of a major stop and a 15 min headway\n",
    "    # but not a 30 min headway. Others will only be in a 15 min headway buffer\n",
    "    # we group areas that share the same combination. We then use that in the\n",
    "    # naming of those groups just downstream; naming them based on\n",
    "    # the boolean True values (indicating a particular buffer is represented)\n",
    "\n",
    "    union_4['ngroup'] = union_4.groupby(source_frames_ordered).ngroup()\n",
    "\n",
    "    # Then, for each combination of overlapping areas, get the most important\n",
    "    # where rail (major stop) trumps 15min headway; 15min headway trumps 30 min headway etc\n",
    "\n",
    "    union_4['cat5'] = union_4.ngroup.map(union_4.groupby(['ngroup'])[source_frames_ordered].apply(\n",
    "        get_combination_names_hierarchy))\n",
    "\n",
    "    # get the full string list of components in any given unioned area\n",
    "    union_4['ngroup_desc'] = union_4.ngroup.map(union_4.groupby(['ngroup'])[source_frames_ordered].apply(\n",
    "        get_combination_names))\n",
    "\n",
    "    if write_to_disk:\n",
    "        union_4.to_file(f'/Users/aolsen/Downloads/union_4_{scenario}.geojson',driver='GeoJSON')\n",
    "\n",
    "    return union_4, union_4.dissolve('cat5', as_index=False)[['cat5', 'geometry']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "id": "1258af15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['EIR runs/Baseline Large (s25) runs/NP_v8_FINAL/run314',\n",
       " 'EIR runs/Alt1 (s26) runs/Alt1_v3_test_far_tiers_FINAL_EIR_ALT/run375',\n",
       " 'EIR runs/Alt2 (s28) runs/Alt2_v1_FINAL_EIR_ALT/run374',\n",
       " 'Final Blueprint runs/Final Blueprint (s24)/BAUS v2.25 - FINAL VERSION/run182']"
      ]
     },
     "execution_count": 469,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_us_runid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "id": "9361a1bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/21/2024 03:57:09 PM - INFO - \n",
      "03/21/2024 03:57:09 PM - INFO - ==== Processing UrbanSim run EIR runs/Baseline Large (s25) runs/NP_v8_FINAL/run314 ====\n",
      "03/21/2024 03:57:09 PM - INFO - Reading 2015 parcel data from /Users/aolsen/Box/Modeling and Surveys/Urban Modeling/Bay Area UrbanSim/PBA50/EIR runs/Baseline Large (s25) runs/NP_v8_FINAL/run314_parcel_data_2015.csv\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[470], line 23\u001b[0m\n\u001b[1;32m     19\u001b[0m     parcel_file \u001b[38;5;241m=\u001b[39m urbansim_runid \u001b[38;5;241m+\u001b[39m \\\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parcel_data_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(model_year)\n\u001b[1;32m     21\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReading \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m parcel data from \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m     22\u001b[0m     model_year, parcel_file))\n\u001b[0;32m---> 23\u001b[0m parcel_output \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparcel_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpython\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m  Read \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m rows\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mlen\u001b[39m(parcel_output)))\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# keep essential columns\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/geo_env_2/lib/python3.10/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/geo_env_2/lib/python3.10/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/geo_env_2/lib/python3.10/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/geo_env_2/lib/python3.10/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    610\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 611\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/geo_env_2/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1778\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1771\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m     (\n\u001b[1;32m   1775\u001b[0m         index,\n\u001b[1;32m   1776\u001b[0m         columns,\n\u001b[1;32m   1777\u001b[0m         col_dict,\n\u001b[0;32m-> 1778\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1779\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[1;32m   1780\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1782\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/geo_env_2/lib/python3.10/site-packages/pandas/io/parsers/python_parser.py:250\u001b[0m, in \u001b[0;36mPythonParser.read\u001b[0;34m(self, rows)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread\u001b[39m(\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28mself\u001b[39m, rows: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    246\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[\n\u001b[1;32m    247\u001b[0m     Index \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, Sequence[Hashable] \u001b[38;5;241m|\u001b[39m MultiIndex, Mapping[Hashable, ArrayLike]\n\u001b[1;32m    248\u001b[0m ]:\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 250\u001b[0m         content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_lines\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    252\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_first_chunk:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/geo_env_2/lib/python3.10/site-packages/pandas/io/parsers/python_parser.py:1123\u001b[0m, in \u001b[0;36mPythonParser._get_lines\u001b[0;34m(self, rows)\u001b[0m\n\u001b[1;32m   1120\u001b[0m rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1122\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1123\u001b[0m     new_row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_iter_line\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow_num\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrows\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1124\u001b[0m     rows \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1126\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m new_row \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/geo_env_2/lib/python3.10/site-packages/pandas/io/parsers/python_parser.py:786\u001b[0m, in \u001b[0;36mPythonParser._next_iter_line\u001b[0;34m(self, row_num)\u001b[0m\n\u001b[1;32m    783\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    784\u001b[0m     \u001b[38;5;66;03m# assert for mypy, data is Iterator[str] or None, would error in next\u001b[39;00m\n\u001b[1;32m    785\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 786\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    787\u001b[0m     \u001b[38;5;66;03m# for mypy\u001b[39;00m\n\u001b[1;32m    788\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(line, \u001b[38;5;28mlist\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Bring in urbansim results\n",
    "all_prox = pd.DataFrame()\n",
    "taz_prox = pd.DataFrame()\n",
    "\n",
    "for us_runid in list_us_runid:\n",
    "    logger.info(\"\")\n",
    "    logger.info(\"==== Processing UrbanSim run {} ====\".format(us_runid))\n",
    "    urbansim_runid = Path(urbansim_run_location, us_runid)\n",
    "\n",
    "    for model_year in (2015, 2050):\n",
    "        if model_year == 2050:\n",
    "            if us_runid == us_2050_FBP_Final:\n",
    "                parcel_file = urbansim_runid + \\\n",
    "                    '_parcel_data_{}_UBI.csv'.format(model_year)\n",
    "            else:\n",
    "                parcel_file = urbansim_runid + \\\n",
    "                    '_parcel_data_{}.csv'.format(model_year)\n",
    "        else:\n",
    "            parcel_file = urbansim_runid + \\\n",
    "                '_parcel_data_{}.csv'.format(model_year)\n",
    "        logger.info('Reading {} parcel data from {}'.format(\n",
    "            model_year, parcel_file))\n",
    "        parcel_output = pd.read_csv(parcel_file, engine='python')\n",
    "\n",
    "        logger.info('  Read {} rows'.format(len(parcel_output)))\n",
    "\n",
    "        # keep essential columns\n",
    "        parcel_output.drop(['geom_id', 'total_job_spaces', 'zoned_du',\n",
    "                            'zoned_du_underbuild', 'zoned_du_underbuild_nodev', 'first_building_type'], axis=1, inplace=True)\n",
    "        logger.info(\"Head:\\n{}\".format(parcel_output.head()))\n",
    "\n",
    "        parcel_output['totemp'] = parcel_output['totemp'].fillna(0)\n",
    "        parcel_output['totemp'] = parcel_output['totemp'].round(\n",
    "            0).astype('int')\n",
    "        parcel_output['RETEMPN'] = parcel_output['RETEMPN'].fillna(0)\n",
    "        parcel_output['RETEMPN'] = parcel_output['RETEMPN'].round(\n",
    "            0).astype('int')\n",
    "        parcel_output['MWTEMPN'] = parcel_output['MWTEMPN'].fillna(0)\n",
    "        parcel_output['MWTEMPN'] = parcel_output['MWTEMPN'].round(\n",
    "            0).astype('int')\n",
    "\n",
    "        # save as table in gdb\n",
    "#         parcel_table = Path(arcpy.env.workspace, \"parcel_table\")\n",
    "#         if arcpy.Exists(parcel_table): arcpy.management.Delete(parcel_table)\n",
    "\n",
    "#         parcel_array = np.array(np.rec.fromrecords(parcel_output.values))\n",
    "#         parcel_array.dtype.names = tuple(parcel_output.dtypes.index.tolist())\n",
    "#         arcpy.da.NumPyArrayToTable(parcel_array, parcel_table)\n",
    "#         logger.info(\"Saved to {} in {}\".format(parcel_table, arcpy.env.workspace))\n",
    "\n",
    "        # convert to point feature class in GDB\n",
    "#         if arcpy.Exists('parcel_fc'): arcpy.management.Delete('parcel_fc')\n",
    "#         arcpy.management.XYTableToPoint(in_table=parcel_table, out_feature_class='parcel_fc',x_field='x',y_field='y')\n",
    "#         logger.info(\"Saved to {} in {}\".format('parcel_fc', arcpy.env.workspace))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b89affc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da8d712",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_year == 2015:\n",
    "        # current\n",
    "        transit_features = ['trn_cur_cat5']\n",
    "    elif model_year == 2050:\n",
    "        # no plan and blueprint\n",
    "        transit_features = ['trn_np_cat5', 'trn_fp_cat5']\n",
    "\n",
    "    for transit_feature in transit_features:\n",
    "\n",
    "        logger.info('Summarizing {} parcel data proximity to {}'.format(\n",
    "            model_year, transit_feature))\n",
    "        log_workspace_contents(logger)\n",
    "\n",
    "        try:\n",
    "            logger.info(\"feature classes no paths\")\n",
    "            arcpy.SummarizeWithin_analysis(transit_feature, 'parcel_fc', 'prox', keep_all_polygons=\"KEEP_ALL\",\n",
    "                                            sum_fields=[['tothh', 'SUM'], ['hhq1','SUM'],['totemp','SUM'],['RETEMPN','SUM'],['MWTEMPN','SUM']])\n",
    "            # hasn't worked, see comments below\n",
    "            logger.info(\"SUCCESS\")\n",
    "        except:\n",
    "            # Get the tool error messages\n",
    "            msgs = arcpy.GetMessages(2)\n",
    "            logger.error(\"Exception occured; msgs: {}\".format(msgs))\n",
    "\n",
    "            # Get the traceback object\n",
    "            tb = sys.exc_info()[2]\n",
    "            tbinfo = traceback.format_tb(tb)[0]\n",
    "\n",
    "            # Concatenate information together concerning the error into a message string\n",
    "            logger.error(\"Traceback info:\\n{}\\nError Info:\\n{}\".format(\n",
    "                tbinfo, str(sys.exc_info()[1])))\n",
    "            logger.error(\n",
    "                \"It's ok though -- we'll do this another way, but still trying the easy way\")\n",
    "\n",
    "        # Something related to arcpy.SummarizeWithin_analysis() is buggy\n",
    "        # The following attempts have failed with\n",
    "        # ERROR 000187: Only supports Geodatabase tables and feature classes\n",
    "        # * use the method with feature layers as inputs, with full paths and without\n",
    "        # * use the method with feature classes as inputs, with full paths and without\n",
    "        # * call the short script below via subprocess\n",
    "        # * copy feature classes to arcpy.env.scratchGDB and summarizeWithin there\n",
    "        #\n",
    "        # HOWEVER, after this script fails, running the following on the command line succeeds:\n",
    "        #\n",
    "        # >>> import arcpy\n",
    "        # >>> arcpy.env.workspace='M:\\Data\\GIS layers\\JobsHousingTransitProximity\\workspace_2020_1007_1737.gdb'\n",
    "        # >>> arcpy.SummarizeWithin_analysis('trn_cur_cat5', 'parcel_fc', 'prox', keep_all_polygons='KEEP_ALL', sum_fields=[['tothh','SUM'], ['hhq1','SUM'],['totemp','SUM'],['RETEMPN','SUM'],['MWTEMPN','SUM']])\n",
    "        # <Result 'M:\\\\Data\\\\GIS layers\\\\JobsHousingTransitProximity\\\\workspace_2020_1007_1737.gdb\\\\prox'>\n",
    "        # >>> prox_sdf = pd.DataFrame.spatial.from_featureclass('prox')\n",
    "        #\n",
    "        # so we'll summarize within ourselves and use spatial join instead\n",
    "        if arcpy.Exists('parcel_fc_join_trn'):\n",
    "            arcpy.management.Delete('parcel_fc_join_trn')\n",
    "        logger.info(\"spatial joining parcel_fc with {}\".format(transit_feature))\n",
    "        arcpy.SpatialJoin_analysis(target_features='parcel_fc', join_features=transit_feature,\n",
    "                                   out_feature_class='parcel_fc_join_trn')\n",
    "\n",
    "        logger.info(\"spatial joining parcel_fc_join_trn with {}\".format(taz))\n",
    "        arcpy.SpatialJoin_analysis(target_features='parcel_fc_join_trn', join_features=taz,\n",
    "                                   out_feature_class='parcel_fc_join_trn_taz')\n",
    "\n",
    "        logger.info(\n",
    "            \"spatial joining parcel_fc_join_trn_taz with {}\".format(tract))\n",
    "        arcpy.SpatialJoin_analysis(target_features='parcel_fc_join_trn_taz', join_features=tract,\n",
    "                                   out_feature_class='parcel_fc_join_trn_taz_tract')\n",
    "        logger.info(\"    ...complete\")\n",
    "\n",
    "        prox_sdf_ba = pd.DataFrame.spatial.from_featureclass(\n",
    "            'parcel_fc_join_trn_taz_tract')\n",
    "        prox_sdf = prox_sdf_ba.groupby('Service_Level').agg({'tothh': 'sum', 'hhq1':'sum', \n",
    "                                                         'totemp': 'sum', 'RETEMPN':'sum',\n",
    "                                                         'MWTEMPN': 'sum'}).reset_index()\n",
    "\n",
    "        prox_sdf['tothh_share'  ] = round(prox_sdf.tothh  / prox_sdf.tothh.sum(),  2)\n",
    "        prox_sdf['hhq1_share'   ] = round(prox_sdf.hhq1   / prox_sdf.hhq1.sum()  , 2)\n",
    "        prox_sdf['totemp_share' ] = round(prox_sdf.totemp / prox_sdf.totemp.sum(), 2)\n",
    "        prox_sdf['RETEMPN_share'] = round(prox_sdf.RETEMPN/prox_sdf.RETEMPN.sum(), 2)\n",
    "        prox_sdf['MWTEMPN_share'] = round(prox_sdf.MWTEMPN/prox_sdf.MWTEMPN.sum(), 2)\n",
    "        prox_sdf['year'] = str(model_year)\n",
    "        prox_sdf['modelrunID'] = us_runid\n",
    "        prox_sdf['transit'] = transit_feature\n",
    "        prox_sdf['area'] = 'Region'\n",
    "\n",
    "        logger.info(\"prox_sdf:\\n{}\".format(prox_sdf))\n",
    "        all_prox = all_prox.append(prox_sdf)\n",
    "\n",
    "        prox_sdf_epc = prox_sdf_ba[prox_sdf_ba.epc == 1]\n",
    "        prox_sdf_epc = prox_sdf_epc.groupby('Service_Level').agg({'tothh': 'sum', 'hhq1':'sum', \n",
    "                                                         'totemp': 'sum', 'RETEMPN':'sum',\n",
    "                                                         'MWTEMPN': 'sum'}).reset_index()\n",
    "\n",
    "        prox_sdf_epc['tothh_share'  ] = round(prox_sdf_epc.tothh  / prox_sdf_epc.tothh.sum(),  2)\n",
    "        prox_sdf_epc['hhq1_share'   ] = round(prox_sdf_epc.hhq1   / prox_sdf_epc.hhq1.sum()  , 2)\n",
    "        prox_sdf_epc['totemp_share' ] = round(prox_sdf_epc.totemp / prox_sdf_epc.totemp.sum(), 2)\n",
    "        prox_sdf_epc['RETEMPN_share'] = round(prox_sdf_epc.RETEMPN/prox_sdf_epc.RETEMPN.sum(), 2)\n",
    "        prox_sdf_epc['MWTEMPN_share'] = round(prox_sdf_epc.MWTEMPN/prox_sdf_epc.MWTEMPN.sum(), 2)\n",
    "        prox_sdf_epc['year'] = str(model_year)\n",
    "        prox_sdf_epc['modelrunID'] = us_runid\n",
    "        prox_sdf_epc['transit'] = transit_feature\n",
    "        prox_sdf_epc['area'] = 'epcs'\n",
    "\n",
    "        logger.info(\"prox_sdf_epc:\\n{}\".format(prox_sdf_epc))\n",
    "        all_prox = all_prox.append(prox_sdf_epc)\n",
    "\n",
    "        prox_sdf_hra = prox_sdf_ba[prox_sdf_ba.hra == 1]\n",
    "        prox_sdf_hra = prox_sdf_hra.groupby('Service_Level').agg({'tothh': 'sum', 'hhq1':'sum', \n",
    "                                                         'totemp': 'sum', 'RETEMPN':'sum',\n",
    "                                                         'MWTEMPN': 'sum'}).reset_index()\n",
    "\n",
    "        prox_sdf_hra['tothh_share'  ] = round(prox_sdf_hra.tothh  / prox_sdf_hra.tothh.sum(),  2)\n",
    "        prox_sdf_hra['hhq1_share'   ] = round(prox_sdf_hra.hhq1   / prox_sdf_hra.hhq1.sum()  , 2)\n",
    "        prox_sdf_hra['totemp_share' ] = round(prox_sdf_hra.totemp / prox_sdf_hra.totemp.sum(), 2)\n",
    "        prox_sdf_hra['RETEMPN_share'] = round(prox_sdf_hra.RETEMPN/prox_sdf_hra.RETEMPN.sum(), 2)\n",
    "        prox_sdf_hra['MWTEMPN_share'] = round(prox_sdf_hra.MWTEMPN/prox_sdf_hra.MWTEMPN.sum(), 2)\n",
    "        prox_sdf_hra['year'] = str(model_year)\n",
    "        prox_sdf_hra['modelrunID'] = us_runid\n",
    "        prox_sdf_hra['transit'] = transit_feature\n",
    "        prox_sdf_hra['area'] = 'HRAs'\n",
    "\n",
    "        logger.info(\"prox_sdf_hra:\\n{}\".format(prox_sdf_hra))\n",
    "        all_prox = all_prox.append(prox_sdf_hra)\n",
    "\n",
    "        logger.info(\"all_prox:\\n{}\".format(all_prox))\n",
    "\n",
    "        prox_sdf_taz = prox_sdf_ba.groupby(['area_type', 'Service_Level']).agg({'tothh':'sum', 'hhq1':'sum', \n",
    "                                                         'totemp': 'sum', 'RETEMPN':'sum',\n",
    "                                                         'MWTEMPN': 'sum'}).reset_index()\n",
    "\n",
    "        prox_sdf_taz['tothh_share'  ] = round(prox_sdf_taz.tothh  / prox_sdf_taz.tothh.sum(),  2)\n",
    "        prox_sdf_taz['hhq1_share'   ] = round(prox_sdf_taz.hhq1   / prox_sdf_taz.hhq1.sum()  , 2)\n",
    "        prox_sdf_taz['totemp_share' ] = round(prox_sdf_taz.totemp / prox_sdf_taz.totemp.sum(), 2)\n",
    "        prox_sdf_taz['RETEMPN_share'] = round(prox_sdf_taz.RETEMPN/prox_sdf_taz.RETEMPN.sum(), 2)\n",
    "        prox_sdf_taz['MWTEMPN_share'] = round(prox_sdf_taz.MWTEMPN/prox_sdf_taz.MWTEMPN.sum(), 2)\n",
    "        prox_sdf_taz['year'] = str(model_year)\n",
    "        prox_sdf_taz['modelrunID'] = us_runid\n",
    "        prox_sdf_taz['transit'] = transit_feature\n",
    "\n",
    "        logger.info(\"prox_sdf:\\n{}\".format(prox_sdf_taz))\n",
    "        taz_prox = taz_prox.append(prox_sdf_taz)\n",
    "\n",
    "        logger.info(\"taz_prox:\\n{}\".format(taz_prox))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6201ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "d727ff6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# union_1.to_file('/Users/aolsen/Downloads/union_1.geojson', driver='GeoJSON')\n",
    "# union_2.to_file('/Users/aolsen/Downloads/union_2.geojson', driver='GeoJSON')\n",
    "# union_3.to_file('/Users/aolsen/Downloads/union_3.geojson', driver='GeoJSON')\n",
    "# union_4.to_file('/Users/aolsen/Downloads/union_4.geojson', driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "60e55196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hdwy15_buf.to_file(\n",
    "#     '/Users/aolsen/Downloads/hdwy15_buf.geojson', driver='GeoJSON')\n",
    "# hdwy30_buf.to_file(\n",
    "#     '/Users/aolsen/Downloads/hdwy30_buf.geojson', driver='GeoJSON')\n",
    "# hdwy30plus_buf.to_file(\n",
    "#     '/Users/aolsen/Downloads/hdwy30plus_buf.geojson', driver='GeoJSON')\n",
    "# major_buf.to_file(\n",
    "#     '/Users/aolsen/Downloads/major_buf.geojson', driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "07dfcdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# diff_1 = gpd.overlay(hdwy15_buf,major_buf,how='difference')\n",
    "\n",
    "# diff_2 = gpd.overlay(hdwy30_buf,major_buf,how='difference')\n",
    "# diff_2b = gpd.overlay(diff_2,hdwy15_buf,how='difference')\n",
    "\n",
    "# diff_3 = gpd.overlay(hdwy30plus_buf,major_buf,how='difference')\n",
    "# diff_3b = gpd.overlay(diff_3,hdwy15_buf,how='difference')\n",
    "# diff_3c = gpd.overlay(diff_3b,hdwy30_buf,how='difference')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578c7118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check prefix+'_cat5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9cdcda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973ea64a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e77675",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d90d5d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b877de",
   "metadata": {},
   "outputs": [],
   "source": [
    "    logger.info(\"==== create_transit_features({}) ====\".format(transit_type))\n",
    "    if transit_type == \"current\":\n",
    "        input_layer = \"transit_current\"\n",
    "        prefix = \"trn_cur\"\n",
    "    elif transit_type == \"noplan\":\n",
    "        input_layer = \"transit_potential\"\n",
    "        prefix = \"trn_np\"\n",
    "        curprefix = \"trn_cur\"\n",
    "    elif transit_type == \"blueprint\":\n",
    "        input_layer = \"transit_potential\"\n",
    "        prefix = \"trn_fp\"\n",
    "        curprefix = \"trn_cur\"\n",
    "    else:\n",
    "        logger.fatal(\"Unsupported transit_type {}\".format(transit_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "b15792c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "transit_type_config = {\n",
    "    \"current\": {\n",
    "        \"input_layer\": \"transit_current\",\n",
    "        \"prefix\": \"trn_cur\",\n",
    "    },\n",
    "    \"noplan\": {\n",
    "        \"input_layer\": \"transit_potential\",\n",
    "        \"prefix\": \"trn_np\",\n",
    "        \"curprefix\": \"trn_cur\",\n",
    "    },\n",
    "    \"blueprint\": {\n",
    "        \"input_layer\": \"transit_potential\",\n",
    "        \"prefix\": \"trn_fp\",\n",
    "        \"curprefix\": \"trn_cur\",\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38a58e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_transit_features_new(logger, transit_type):\n",
    "    \"\"\"\n",
    "    transit_type is one of [current, noplan, blueprint]\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    orig_result = arcpy.GetCount_management(input_layer)\n",
    "    logger.info(\"{} has {} rows\".format(input_layer, orig_result[0]))\n",
    "    \n",
    "    \n",
    "    if transit_type == \"current\":\n",
    "        logger.info('Select buffer for major transit stops => {}_majorbuf'.format(prefix))\n",
    "        major = arcpy.SelectLayerByAttribute_management(input_layer, \"NEW_SELECTION\", '\"major_stop\" = 1')\n",
    "        arcpy.CopyFeatures_management(major, prefix+\"_major\")  # save selection to new feature class\n",
    "        major_result = arcpy.GetCount_management(prefix+\"_major\")\n",
    "        logger.info(\"  {}_major has {} rows\".format(prefix, major_result[0]))\n",
    "        arcpy.Buffer_analysis(prefix+\"_major\", prefix+\"_majorbuf\", \"0.5 Miles\", \"FULL\", \"ROUND\", \"ALL\", None, \"PLANAR\")\n",
    "   \n",
    "    else:\n",
    "        if transit_type==\"noplan\":\n",
    "            logger.info('Selecting \"Under Construction\" or \"Open\" stops for no plan')\n",
    "            new_major = arcpy.management.SelectLayerByAttribute(input_layer, \"NEW_SELECTION\", \n",
    "                                                                '\"status\" = \\'Under Construction\\' Or status=\\'Open\\'')\n",
    "        elif transit_type==\"blueprint\":\n",
    "            logger.info('Selecting \"Under Construction\" or \"Open\" or \"Final Blueprint\" stops for no plan')\n",
    "            new_major = arcpy.management.SelectLayerByAttribute(input_layer, \"NEW_SELECTION\", \n",
    "                                                                '\"status\" = \\'Under Construction\\' Or status=\\'Open\\' Or status=\\'Final Blueprint\\'')\n",
    "\n",
    "        arcpy.CopyFeatures_management(new_major, prefix+\"_new_major\")  # save selection to new feature class\n",
    "        new_major_result = arcpy.GetCount_management(prefix+\"_new_major\")\n",
    "        logger.info(\"  {}_new_major has {} rows\".format(prefix, new_major_result[0]))\n",
    "        arcpy.Buffer_analysis(prefix+\"_new_major\", prefix+\"_newmajorbuf\", \"0.5 Miles\", \"FULL\", \"ROUND\", \"ALL\", None, \"PLANAR\")\n",
    "\n",
    "        # merge new major buffered with original major buffered\n",
    "        arcpy.Merge_management([curprefix+\"_majorbuf\",prefix+\"_newmajorbuf\"], prefix+\"_majorbuf_predissolve\")\n",
    "        # dissolve\n",
    "        arcpy.management.Dissolve(prefix+\"_majorbuf_predissolve\", prefix+\"_majorbuf\", \n",
    "                                  dissolve_field=None, statistics_fields=None, multi_part=\"MULTI_PART\", unsplit_lines=\"DISSOLVE_LINES\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d81b185",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_transit_features(logger, transit_type):\n",
    "    \"\"\"\n",
    "    transit_type is one of [current, noplan, blueprint]\n",
    "    \"\"\"\n",
    "    logger.info(\"==== create_transit_features({}) ====\".format(transit_type))\n",
    "    if transit_type == \"current\":\n",
    "        input_layer = \"transit_current\"\n",
    "        prefix = \"trn_cur\"\n",
    "    elif transit_type == \"noplan\":\n",
    "        input_layer = \"transit_potential\"\n",
    "        prefix = \"trn_np\"\n",
    "        curprefix = \"trn_cur\"\n",
    "    elif transit_type == \"blueprint\":\n",
    "        input_layer = \"transit_potential\"\n",
    "        prefix = \"trn_fp\"\n",
    "        curprefix = \"trn_cur\"\n",
    "    else:\n",
    "        logger.fatal(\"Unsupported transit_type {}\".format(transit_type))\n",
    "\n",
    "    orig_result = arcpy.GetCount_management(input_layer)\n",
    "    logger.info(\"{} has {} rows\".format(input_layer, orig_result[0]))\n",
    "\n",
    "    # buffered transit_current major stops\n",
    "    if transit_type == \"current\":\n",
    "        logger.info(\n",
    "            'Select buffer for major transit stops => {}_majorbuf'.format(prefix))\n",
    "        major = arcpy.SelectLayerByAttribute_management(\n",
    "            input_layer, \"NEW_SELECTION\", '\"major_stop\" = 1')\n",
    "        # save selection to new feature class\n",
    "        arcpy.CopyFeatures_management(major, prefix+\"_major\")\n",
    "        major_result = arcpy.GetCount_management(prefix+\"_major\")\n",
    "        logger.info(\"  {}_major has {} rows\".format(prefix, major_result[0]))\n",
    "        arcpy.Buffer_analysis(prefix+\"_major\", prefix+\"_majorbuf\",\n",
    "                              \"0.5 Miles\", \"FULL\", \"ROUND\", \"ALL\", None, \"PLANAR\")\n",
    "\n",
    "    else:\n",
    "        if transit_type == \"noplan\":\n",
    "            logger.info(\n",
    "                'Selecting \"Under Construction\" or \"Open\" stops for no plan')\n",
    "            new_major = arcpy.management.SelectLayerByAttribute(input_layer, \"NEW_SELECTION\",\n",
    "                                                                '\"status\" = \\'Under Construction\\' Or status=\\'Open\\'')\n",
    "        elif transit_type == \"blueprint\":\n",
    "            logger.info(\n",
    "                'Selecting \"Under Construction\" or \"Open\" or \"Final Blueprint\" stops for no plan')\n",
    "            new_major = arcpy.management.SelectLayerByAttribute(input_layer, \"NEW_SELECTION\",\n",
    "                                                                '\"status\" = \\'Under Construction\\' Or status=\\'Open\\' Or status=\\'Final Blueprint\\'')\n",
    "\n",
    "        # save selection to new feature class\n",
    "        arcpy.CopyFeatures_management(new_major, prefix+\"_new_major\")\n",
    "        new_major_result = arcpy.GetCount_management(prefix+\"_new_major\")\n",
    "        logger.info(\"  {}_new_major has {} rows\".format(\n",
    "            prefix, new_major_result[0]))\n",
    "        arcpy.Buffer_analysis(prefix+\"_new_major\", prefix+\"_newmajorbuf\",\n",
    "                              \"0.5 Miles\", \"FULL\", \"ROUND\", \"ALL\", None, \"PLANAR\")\n",
    "\n",
    "        # merge new major buffered with original major buffered\n",
    "        arcpy.Merge_management(\n",
    "            [curprefix+\"_majorbuf\", prefix+\"_newmajorbuf\"], prefix+\"_majorbuf_predissolve\")\n",
    "        # dissolve\n",
    "        arcpy.management.Dissolve(prefix+\"_majorbuf_predissolve\", prefix+\"_majorbuf\",\n",
    "                                  dissolve_field=None, statistics_fields=None, multi_part=\"MULTI_PART\", unsplit_lines=\"DISSOLVE_LINES\")\n",
    "\n",
    "    # buffered hdway_15min stops\n",
    "    logger.info(\n",
    "        'Creating buffer for stops with headway < 15min => {}_hdwy15buf'.format(prefix))\n",
    "\n",
    "    if transit_type == \"current\":\n",
    "        hdwy15 = arcpy.SelectLayerByAttribute_management(\n",
    "            input_layer, \"NEW_SELECTION\", \"am_av_hdwy <= 15 And pm_av_hdwy <= 15\")\n",
    "        # save selection to new feature class\n",
    "        arcpy.CopyFeatures_management(hdwy15, prefix+\"_hdwy15\")\n",
    "        hdwy15_result = arcpy.GetCount_management(prefix+\"_hdwy15\")\n",
    "        logger.info(\"  {}_hdwy15 has {} rows\".format(prefix, hdwy15_result[0]))\n",
    "        arcpy.Buffer_analysis(prefix+\"_hdwy15\", prefix+\"_hdwy15buf\",\n",
    "                              \"0.25 Miles\", \"FULL\", \"ROUND\", \"ALL\", None, \"PLANAR\")\n",
    "\n",
    "    else:\n",
    "        # copy current\n",
    "        arcpy.CopyFeatures_management(\n",
    "            curprefix+\"_hdwy15buf\", prefix+\"_hdwy15buf\")\n",
    "\n",
    "    # buffered hdway_30min stops\n",
    "    logger.info(\n",
    "        'Creating buffer for stops with headway 15-30 min => {}_hdwy30buf'.format(prefix))\n",
    "\n",
    "    if transit_type == \"current\":\n",
    "        hdwy30 = arcpy.SelectLayerByAttribute_management(\n",
    "            input_layer, \"NEW_SELECTION\", \"am_av_hdwy > 15 And am_av_hdwy <= 30 And pm_av_hdwy > 15 And pm_av_hdwy <= 30\")\n",
    "        # save selection to new feature class\n",
    "        arcpy.CopyFeatures_management(hdwy30, prefix+\"_hdwy30\")\n",
    "        hdwy30_result = arcpy.GetCount_management(prefix+\"_hdwy30\")\n",
    "        logger.info(\"  {}_hdwy30 has {} rows\".format(prefix, hdwy30_result[0]))\n",
    "        arcpy.Buffer_analysis(prefix+\"_hdwy30\", prefix+\"_hdwy30buf\",\n",
    "                              \"0.25 Miles\", \"FULL\", \"ROUND\", \"ALL\", None, \"PLANAR\")\n",
    "\n",
    "    else:\n",
    "        # copy current\n",
    "        arcpy.CopyFeatures_management(\n",
    "            curprefix+\"_hdwy30buf\", prefix+\"_hdwy30buf\")\n",
    "\n",
    "    # buffered hdway_30plus stops\n",
    "    logger.info(\n",
    "        'Creating buffer for stops with headway 30+ min => {}_hdwy30plusbuf'.format(prefix))\n",
    "\n",
    "    if transit_type == \"current\":\n",
    "        hdwy30plus = arcpy.SelectLayerByAttribute_management(\n",
    "            input_layer, \"NEW_SELECTION\", \"am_av_hdwy > 30 Or pm_av_hdwy > 30\")\n",
    "        # save selection to new feature class\n",
    "        arcpy.CopyFeatures_management(hdwy30plus, prefix+\"_hdwy30plus\")\n",
    "        hdwy30plus_result = arcpy.GetCount_management(prefix+\"_hdwy30plus\")\n",
    "        logger.info(\"  {}_hdwy30plus has {} rows\".format(\n",
    "            prefix, hdwy30plus_result[0]))\n",
    "        arcpy.Buffer_analysis(prefix+\"_hdwy30plus\", prefix+\"_hdwy30plusbuf\",\n",
    "                              \"0.25 Miles\", \"FULL\", \"ROUND\", \"ALL\", None, \"PLANAR\")\n",
    "\n",
    "    else:\n",
    "        # copy current\n",
    "        arcpy.CopyFeatures_management(\n",
    "            curprefix+\"_hdwy30plusbuf\", prefix+\"_hdwy30plusbuf\")\n",
    "\n",
    "    # Make them disjoint -- first one wins\n",
    "    logger.info(\n",
    "        'Isolate {}_hdwy15buf => {}_hdwy15buf_only'.format(prefix, prefix))\n",
    "    arcpy.Erase_analysis(in_features=prefix+\"_hdwy15buf\", erase_features=prefix+\"_majorbuf\",\n",
    "                         out_feature_class=prefix+\"_hdwy15buf_only\")\n",
    "\n",
    "    logger.info(\n",
    "        'Isolate {}_hdwy30buf => {}_hdwy30buf_only'.format(prefix, prefix))\n",
    "    arcpy.Erase_analysis(prefix+\"_hdwy30buf\",   prefix +\n",
    "                         \"_majorbuf\",  prefix+\"_hdwy30buf_1\")\n",
    "    arcpy.Erase_analysis(prefix+\"_hdwy30buf_1\", prefix +\n",
    "                         \"_hdwy15buf\", prefix+\"_hdwy30buf_only\")\n",
    "    arcpy.Delete_management([prefix+\"_hdwy30buf_1\"])\n",
    "\n",
    "    logger.info(\n",
    "        'Isolate {}_hdwy30plusbuf  => {}_hdwy30plusbuf_only'.format(prefix, prefix))\n",
    "    arcpy.Erase_analysis(prefix+\"_hdwy30plusbuf\",   prefix +\n",
    "                         \"_majorbuf\",  prefix+\"_hdwy30plusbuf_1\")\n",
    "    arcpy.Erase_analysis(prefix+\"_hdwy30plusbuf_1\", prefix +\n",
    "                         \"_hdwy15buf\", prefix+\"_hdwy30plusbuf_2\")\n",
    "    arcpy.Erase_analysis(prefix+\"_hdwy30plusbuf_2\", prefix +\n",
    "                         \"_hdwy30buf\", prefix+\"_hdwy30plusbuf_only\")\n",
    "    arcpy.Delete_management([prefix+\"_hdwy30plusbuf_1\",\n",
    "                             prefix+\"_hdwy30plusbuf_2\"])\n",
    "\n",
    "    logger.info('Rest of Bay Area => {}_none'.format(prefix))\n",
    "    arcpy.Erase_analysis(\"BAcounty_expand\",   prefix +\n",
    "                         \"_majorbuf\",      \"BAcounty_expand_1\")\n",
    "    arcpy.Erase_analysis(\"BAcounty_expand_1\", prefix +\n",
    "                         \"_hdwy15buf\",     \"BAcounty_expand_2\")\n",
    "    arcpy.Erase_analysis(\"BAcounty_expand_2\", prefix +\n",
    "                         \"_hdwy30buf\",     \"BAcounty_expand_3\")\n",
    "    arcpy.Erase_analysis(\"BAcounty_expand_3\", prefix +\n",
    "                         \"_hdwy30plusbuf\", prefix+\"_none\")\n",
    "    arcpy.Delete_management([\"BAcounty_expand_1\",\n",
    "                             \"BAcounty_expand_2\",\n",
    "                             \"BAcounty_expand_3\"])\n",
    "\n",
    "    logger.info('Merge into one feature class => {}_cat5'.format(prefix))\n",
    "    arcpy.Merge_management([prefix+\"_none\",\n",
    "                            prefix+\"_hdwy30plusbuf_only\",\n",
    "                            prefix+\"_hdwy30buf_only\",\n",
    "                            prefix+\"_hdwy15buf_only\",\n",
    "                            prefix+\"_majorbuf\"],\n",
    "                           prefix+\"_cat5\", add_source=\"ADD_SOURCE_INFO\")\n",
    "    # create Service_Level from MERGE_SRC\n",
    "    arcpy.AddField_management(\n",
    "        prefix + \"_cat5\", \"Service_Level\", \"TEXT\", \"\", \"\", 200)\n",
    "    with arcpy.da.UpdateCursor(prefix + \"_cat5\", [\"Service_Level\", \"MERGE_SRC\"]) as cursor:\n",
    "        for row in cursor:\n",
    "            if 'none' in row[1]:\n",
    "                row[0] = 'No_Fixed_Route_Transit'\n",
    "            elif 'hdwy30plusbuf' in row[1]:\n",
    "                row[0] = 'Bus_31plus_min'\n",
    "            elif 'hdwy30buf' in row[1]:\n",
    "                row[0] = 'Bus_15_30min'\n",
    "            elif 'hdwy15buf' in row[1]:\n",
    "                row[0] = 'Bus_<15min'\n",
    "            elif 'majorbuf' in row[1]:\n",
    "                row[0] = 'Major_Transit_Stop'\n",
    "            cursor.updateRow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6060432f",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after 'else' statement on line 34 (3996946805.py, line 35)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[6], line 35\u001b[0;36m\u001b[0m\n\u001b[0;31m    if transit_type == \"noplan\":\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after 'else' statement on line 34\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "from shapely.geometry import Point  # for creating point buffers\n",
    "\n",
    "\n",
    "def create_transit_features(logger, transit_type):\n",
    "    \"\"\"\n",
    "    transit_type is one of [current, noplan, blueprint]\n",
    "    \"\"\"\n",
    "    logger.info(\"==== create_transit_features({}) ====\".format(transit_type))\n",
    "\n",
    "    # Input data loading (adjust path and format)\n",
    "    # Replace with correct path and format\n",
    "    input_gdf = gpd.read_file(\"path/to/your/data.shp\")\n",
    "\n",
    "    if transit_type == \"current\":\n",
    "        prefix = \"trn_cur\"\n",
    "        curprefix = \"trn_cur\"\n",
    "    elif transit_type == \"noplan\":\n",
    "        prefix = \"trn_np\"\n",
    "        curprefix = \"trn_cur\"\n",
    "    elif transit_type == \"blueprint\":\n",
    "        prefix = \"trn_fp\"\n",
    "        curprefix = \"trn_cur\"\n",
    "    else:\n",
    "        logger.fatal(\"Unsupported transit_type {}\".format(transit_type))\n",
    "\n",
    "    # Calculate feature counts (use describe method)\n",
    "    feature_counts = input_gdf.describe()\n",
    "    logger.info(f\"{input_layer} has {feature_counts['geometry'].iloc[0]} rows\")\n",
    "\n",
    "    # Select features based on conditions (use query method)\n",
    "    if transit_type == \"current\":\n",
    "        major_gdf = input_gdf.query(\"major_stop == 1\")\n",
    "    else:\n",
    "        if transit_type == \"noplan\":\n",
    "            new_major_gdf = input_gdf.query(\n",
    "              \"(status == 'Under Construction') | (status == 'Open')\"\n",
    "          )\n",
    "    else:\n",
    "        new_major_gdf = input_gdf.query(\n",
    "          \"(status == 'Under Construction') | (status == 'Open') | (status == 'Final Blueprint')\"\n",
    "      )\n",
    "\n",
    "    # Buffer features (use buffer method)\n",
    "    majorbuf_gdf = major_gdf.buffer(distance=0.5, units=\"miles\")\n",
    "    if transit_type != \"current\":\n",
    "        hdwy15buf_gdf = gpd.read_file(\n",
    "            f\"{curprefix}_hdwy15buf.shp\")  # Assuming existing buffer\n",
    "        hdwy30buf_gdf = gpd.read_file(\n",
    "            f\"{curprefix}_hdwy30buf.shp\")  # Assuming existing buffer\n",
    "        hdwy30plusbuf_gdf = gpd.read_file(\n",
    "            f\"{curprefix}_hdwy30plusbuf.shp\")  # Assuming existing buffer\n",
    "\n",
    "    # ... (similar logic for hdwy15, hdwy30, hdwy30plus selections and buffering using query and buffer methods)\n",
    "\n",
    "    # Isolate buffers using difference (use difference method)\n",
    "    if transit_type == \"current\":\n",
    "        hdwy15buf_only_gdf = majorbuf_gdf.difference(hdwy30buf_gdf)\n",
    "        hdwy30buf_only_gdf = (\n",
    "            majorbuf_gdf.difference(\n",
    "                hdwy15buf_gdf).difference(hdwy30plusbuf_gdf)\n",
    "        )\n",
    "        hdwy30plusbuf_only_gdf = majorbuf_gdf.difference(\n",
    "            hdwy15buf_gdf).difference(hdwy30buf_gdf)\n",
    "    else:\n",
    "    # ... (similar logic for isolating buffers using difference for noplan and blueprint)\n",
    "        pass\n",
    "\n",
    "    # Isolate remaining area using difference (use difference method)\n",
    "    none_gdf = gpd.read_file(\"BAcounty_expand.shp\")  # Assuming existing data\n",
    "    none_gdf = none_gdf.difference(majorbuf_gdf)\n",
    "    if transit_type != \"current\":\n",
    "        none_gdf = none_gdf.difference(hdwy15buf_gdf)\n",
    "        none_gdf = none_gdf.difference(hdwy30buf_gdf)\n",
    "        none_gdf = none_gdf.difference(hdwy30plusbuf_gdf)\n",
    "\n",
    "    # Merge into one GeoDataFrame (use concat and GeoDataFrame constructor)\n",
    "    merged_gdf = gpd.GeoDataFrame(pd.concat([\n",
    "      none_gdf, hdwy30plusbuf_only_gdf, hdwy30buf_only_gdf,\n",
    "      hdwy15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e316c9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
